{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_geometric\n",
        "!pip install rdkit\n",
        "!pip install umap-learn\n",
        "!pip install hdbscan\n",
        "!pip install pandas\n",
        "!pip install matplotlib\n",
        "!pip install seaborn\n",
        "!pip install scikit-learn\n",
        "!pip install numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "haTqPuZumtJ_",
        "outputId": "47010edb-c963-4d21-be97-ec466963bff1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.11.14)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2025.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.2.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2025.1.31)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.6.1\n",
            "Collecting rdkit\n",
            "  Downloading rdkit-2024.9.6-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rdkit) (2.0.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from rdkit) (11.1.0)\n",
            "Downloading rdkit-2024.9.6-cp311-cp311-manylinux_2_28_x86_64.whl (34.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.3/34.3 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rdkit\n",
            "Successfully installed rdkit-2024.9.6\n",
            "Requirement already satisfied: umap-learn in /usr/local/lib/python3.11/dist-packages (0.5.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (1.14.1)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (1.6.1)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (0.60.0)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (0.5.13)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from umap-learn) (4.67.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.2->umap-learn) (0.43.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.11/dist-packages (from pynndescent>=0.5->umap-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22->umap-learn) (3.6.0)\n",
            "Requirement already satisfied: hdbscan in /usr/local/lib/python3.11/dist-packages (0.8.40)\n",
            "Requirement already satisfied: numpy<3,>=1.20 in /usr/local/lib/python3.11/dist-packages (from hdbscan) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.11/dist-packages (from hdbscan) (1.14.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.11/dist-packages (from hdbscan) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from hdbscan) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20->hdbscan) (3.6.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.11/dist-packages (from seaborn) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.11/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.11/dist-packages (from seaborn) (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IbSgaDMdlKp5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "\n",
        "from data_loaders import preproccess_data, generate_scaffold_split, df_to_graph_list, get_scaffolds\n",
        "from gcn_change4 import GCN\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjoE_254nOdP",
        "outputId": "84b0386b-36f7-42cf-f036-ecee05753e57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yVfX36w1lKp9",
        "outputId": "438c7f5f-1473-45fc-b0ee-7242a84d70c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[19:39:20] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:20] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:20] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:20] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:20] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:20] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:20] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:20] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:20] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:20] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:20] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:20] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:20] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:20] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:20] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:20] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:20] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:20] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:20] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:20] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:20] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:20] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:20] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:20] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:20] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:20] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:20] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:20] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:20] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:20] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:20] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:20] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:20] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:20] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:20] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:20] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:20] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:20] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:20] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:21] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:21] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:21] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:21] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:21] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:21] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:21] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:21] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:21] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:21] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:21] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:21] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:21] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:21] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:21] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:21] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:21] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:21] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:21] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:21] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:21] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:21] Explicit valence for atom # 5 N, 4, is greater than permitted\n",
            "[19:39:21] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:21] Explicit valence for atom # 5 N, 4, is greater than permitted\n",
            "[19:39:21] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:21] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:22] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:22] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:23] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:23] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:23] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:23] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:23] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:23] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:23] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:23] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:23] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:23] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:23] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:23] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:23] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:23] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:23] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:23] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:23] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:23] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:23] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:23] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:23] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:23] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:23] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:23] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:23] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:23] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:23] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:23] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:23] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:23] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:23] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:23] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:23] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:23] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:23] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:23] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:23] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:23] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:23] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:23] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:23] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:23] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:23] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:23] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:23] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:23] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:23] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:23] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:23] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:23] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:23] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:23] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:23] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:23] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:23] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:23] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:23] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:23] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:23] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:23] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:24] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:24] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:24] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:24] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:39:24] WARNING: not removing hydrogen atom without neighbors\n"
          ]
        }
      ],
      "source": [
        "file_path = '/content/drive/MyDrive/BioSolveAI/data/curated-solubility-dataset.csv'\n",
        "df = preproccess_data(file_path)\n",
        "\n",
        "\n",
        "df['scaffold'] = df['mol'].apply(get_scaffolds)\n",
        "\n",
        "# scaffolds to get train, val, text\n",
        "train_idx, val_idx, test_idx = generate_scaffold_split(df)\n",
        "\n",
        "# Split the dataframe into train, val, and test\n",
        "train_df = df.iloc[train_idx]\n",
        "val_df = df.iloc[val_idx]\n",
        "test_df = df.iloc[test_idx]\n",
        "\n",
        "# df to graph list\n",
        "train_graph_list = df_to_graph_list(train_df)\n",
        "val_graph_list = df_to_graph_list(val_df)\n",
        "test_graph_list = df_to_graph_list(test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rW1qbtrulKp-"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_graph_list, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_graph_list, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_graph_list, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below run introduces AdamW opt and CosineAnnealing schduler. Also reduced epochs to 50"
      ],
      "metadata": {
        "id": "XF69md-DrLtL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-53tkilllKp-",
        "outputId": "7f95bc07-d62f-4ae0-99f3-bf0aea8ba616"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Train Loss: 330.7809, Val RMSE: 4.1775, R²: -19.3666, CI (95%):[3.8518, 4.4797]\n",
            "Epoch: 2, Train Loss: 35.9041, Val RMSE: 3.1633, R²: -10.6775, CI (95%):[2.8376, 3.4584]\n",
            "Epoch: 3, Train Loss: 20.5847, Val RMSE: 2.8952, R²: -8.7819, CI (95%):[2.6639, 3.1093]\n",
            "Epoch: 4, Train Loss: 12.3498, Val RMSE: 2.1423, R²: -4.3562, CI (95%):[1.9796, 2.2935]\n",
            "Epoch: 5, Train Loss: 7.0016, Val RMSE: 1.9648, R²: -3.5054, CI (95%):[1.7974, 2.1191]\n",
            "Epoch: 6, Train Loss: 4.4244, Val RMSE: 1.5054, R²: -1.6446, CI (95%):[1.4134, 1.5920]\n",
            "Epoch: 7, Train Loss: 2.4298, Val RMSE: 0.9901, R²: -0.1439, CI (95%):[0.9136, 1.0611]\n",
            "Epoch: 8, Train Loss: 1.4540, Val RMSE: 0.8128, R²: 0.2290, CI (95%):[0.7520, 0.8693]\n",
            "Epoch: 9, Train Loss: 1.0414, Val RMSE: 0.7173, R²: 0.3995, CI (95%):[0.6696, 0.7621]\n",
            "Epoch: 10, Train Loss: 0.8091, Val RMSE: 0.6784, R²: 0.4629, CI (95%):[0.6352, 0.7190]\n",
            "Epoch: 11, Train Loss: 0.7077, Val RMSE: 0.6367, R²: 0.5269, CI (95%):[0.6040, 0.6678]\n",
            "Epoch: 12, Train Loss: 0.6353, Val RMSE: 0.6173, R²: 0.5553, CI (95%):[0.5859, 0.6472]\n",
            "Epoch: 13, Train Loss: 0.5895, Val RMSE: 0.6127, R²: 0.5619, CI (95%):[0.5843, 0.6399]\n",
            "Epoch: 14, Train Loss: 0.5624, Val RMSE: 0.5865, R²: 0.5985, CI (95%):[0.5571, 0.6146]\n",
            "Epoch: 15, Train Loss: 0.5294, Val RMSE: 0.6147, R²: 0.5591, CI (95%):[0.5844, 0.6435]\n",
            "Epoch: 16, Train Loss: 0.5065, Val RMSE: 0.5979, R²: 0.5828, CI (95%):[0.5684, 0.6261]\n",
            "Epoch: 17, Train Loss: 0.4883, Val RMSE: 0.6208, R²: 0.5503, CI (95%):[0.5908, 0.6493]\n",
            "Epoch: 18, Train Loss: 0.4739, Val RMSE: 0.5978, R²: 0.5830, CI (95%):[0.5680, 0.6261]\n",
            "Epoch: 19, Train Loss: 0.4699, Val RMSE: 0.5868, R²: 0.5981, CI (95%):[0.5578, 0.6145]\n",
            "Epoch: 20, Train Loss: 0.4582, Val RMSE: 0.5834, R²: 0.6028, CI (95%):[0.5555, 0.6100]\n",
            "Epoch: 21, Train Loss: 0.4564, Val RMSE: 0.7617, R²: 0.3230, CI (95%):[0.7286, 0.7934]\n",
            "Epoch: 22, Train Loss: 0.4474, Val RMSE: 0.6143, R²: 0.5597, CI (95%):[0.5852, 0.6420]\n",
            "Epoch: 23, Train Loss: 0.5304, Val RMSE: 0.5795, R²: 0.6081, CI (95%):[0.5501, 0.6076]\n",
            "Epoch: 24, Train Loss: 0.4481, Val RMSE: 0.5669, R²: 0.6249, CI (95%):[0.5378, 0.5946]\n",
            "Epoch: 25, Train Loss: 0.4156, Val RMSE: 0.5587, R²: 0.6357, CI (95%):[0.5305, 0.5855]\n",
            "Epoch: 26, Train Loss: 0.4319, Val RMSE: 0.5506, R²: 0.6462, CI (95%):[0.5228, 0.5771]\n",
            "Epoch: 27, Train Loss: 0.4247, Val RMSE: 0.5474, R²: 0.6503, CI (95%):[0.5178, 0.5755]\n",
            "Epoch: 28, Train Loss: 0.3965, Val RMSE: 0.5699, R²: 0.6210, CI (95%):[0.5404, 0.5980]\n",
            "Epoch: 29, Train Loss: 0.3886, Val RMSE: 0.5528, R²: 0.6433, CI (95%):[0.5256, 0.5788]\n",
            "Epoch: 30, Train Loss: 0.3887, Val RMSE: 0.5756, R²: 0.6133, CI (95%):[0.5470, 0.6029]\n",
            "Epoch: 31, Train Loss: 0.3710, Val RMSE: 0.5677, R²: 0.6238, CI (95%):[0.5380, 0.5960]\n",
            "Epoch: 32, Train Loss: 0.3667, Val RMSE: 0.5776, R²: 0.6106, CI (95%):[0.5484, 0.6054]\n",
            "Epoch: 33, Train Loss: 0.3666, Val RMSE: 0.5607, R²: 0.6332, CI (95%):[0.5317, 0.5882]\n",
            "Epoch: 34, Train Loss: 0.3560, Val RMSE: 0.5556, R²: 0.6397, CI (95%):[0.5265, 0.5833]\n",
            "Epoch: 35, Train Loss: 0.3744, Val RMSE: 0.5492, R²: 0.6481, CI (95%):[0.5216, 0.5754]\n",
            "Epoch: 36, Train Loss: 0.3462, Val RMSE: 0.5498, R²: 0.6473, CI (95%):[0.5214, 0.5767]\n",
            "Epoch: 37, Train Loss: 0.3415, Val RMSE: 0.5696, R²: 0.6214, CI (95%):[0.5401, 0.5976]\n",
            "Epoch: 38, Train Loss: 0.3417, Val RMSE: 0.5593, R²: 0.6350, CI (95%):[0.5285, 0.5884]\n",
            "Epoch: 39, Train Loss: 0.3365, Val RMSE: 0.5426, R²: 0.6564, CI (95%):[0.5149, 0.5690]\n",
            "Epoch: 40, Train Loss: 0.3264, Val RMSE: 0.5388, R²: 0.6612, CI (95%):[0.5107, 0.5655]\n",
            "Epoch: 41, Train Loss: 0.3333, Val RMSE: 0.5482, R²: 0.6493, CI (95%):[0.5198, 0.5751]\n",
            "Epoch: 42, Train Loss: 0.3274, Val RMSE: 0.5395, R²: 0.6603, CI (95%):[0.5109, 0.5667]\n",
            "Epoch: 43, Train Loss: 0.3155, Val RMSE: 0.5433, R²: 0.6556, CI (95%):[0.5142, 0.5709]\n",
            "Epoch: 44, Train Loss: 0.3135, Val RMSE: 0.5477, R²: 0.6499, CI (95%):[0.5191, 0.5749]\n",
            "Epoch: 45, Train Loss: 0.3106, Val RMSE: 0.5311, R²: 0.6708, CI (95%):[0.5021, 0.5586]\n",
            "Epoch: 46, Train Loss: 0.3069, Val RMSE: 0.5480, R²: 0.6495, CI (95%):[0.5190, 0.5756]\n",
            "Epoch: 47, Train Loss: 0.3070, Val RMSE: 0.5466, R²: 0.6514, CI (95%):[0.5180, 0.5737]\n",
            "Epoch: 48, Train Loss: 0.3036, Val RMSE: 0.5495, R²: 0.6476, CI (95%):[0.5199, 0.5776]\n",
            "Epoch: 49, Train Loss: 0.3025, Val RMSE: 0.5476, R²: 0.6501, CI (95%):[0.5186, 0.5750]\n",
            "Epoch: 50, Train Loss: 0.3018, Val RMSE: 0.5465, R²: 0.6515, CI (95%):[0.5174, 0.5741]\n"
          ]
        }
      ],
      "source": [
        "# Set seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "num_node = train_graph_list[0].x.shape[1]\n",
        "edge_attr = train_graph_list[0].edge_attr.shape[1]\n",
        "u_d = train_graph_list[0].u.shape[1]\n",
        "\n",
        "model = GCN(num_node_features=num_node,\n",
        "            edge_attr_dim=edge_attr,\n",
        "            u_dim=u_d,\n",
        "            hidden_dim=64,\n",
        "            output_dim=1).to(device)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay= 0.01) # introduced AdamW\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-6) # introduced Cosine Annealign Scheduler\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 50\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for data in train_loader:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        target = data.y.view(data.num_graphs, -1).to(device)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * data.num_graphs\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "\n",
        "    # Validation step\n",
        "    model.eval()\n",
        "    all_preds, all_targets = [], []\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for data in val_loader:\n",
        "            data = data.to(device)\n",
        "            output = model(data)\n",
        "            target = data.y.view(data.num_graphs, -1).to(device)\n",
        "            loss = criterion(output, target) #get loss based on criterion\n",
        "            val_loss += loss.item() * data.num_graphs\n",
        "            all_preds.extend(output.cpu().numpy())\n",
        "            all_targets.extend(target.cpu().numpy())\n",
        "    val_loss /= len(val_loader.dataset) #compute validation loss\n",
        "    val_rmse = val_loss ** 0.5\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    # Compute R^2\n",
        "    all_preds = np.array(all_preds).flatten()\n",
        "    all_targets = np.array(all_targets).flatten()\n",
        "    r2 = r2_score(all_targets, all_preds)\n",
        "\n",
        "    # Compute 95% Confidence Interval for RMSE\n",
        "    confidence = 0.95\n",
        "    squared_errors = (all_preds - all_targets) ** 2\n",
        "    mean_se = np.mean(squared_errors)\n",
        "    se = stats.sem(squared_errors)\n",
        "    interval = stats.t.interval(confidence, len(squared_errors)-1, loc=mean_se, scale=se)\n",
        "    ci_lower, ci_upper = np.sqrt(interval[0]), np.sqrt(interval[1])\n",
        "\n",
        "    print(f\"Epoch: {epoch}, Train Loss: {train_loss:.4f}, Val RMSE: {val_rmse:.4f}, R²: {r2:.4f}, CI (95%):[{ci_lower:.4f}, {ci_upper:.4f}]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxF2JHg_lKp_",
        "outputId": "8b10e2a0-535b-4c3b-b38a-27cbf8df30b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test RMSE: 0.4954, R²: 0.6317, CI (95%): [0.4708, 0.5188]\n"
          ]
        }
      ],
      "source": [
        "# Testing\n",
        "model.eval()\n",
        "test_loss = 0\n",
        "all_preds, all_targets = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        data = data.to(device)\n",
        "        output = model(data)\n",
        "        target = data.y.view(data.num_graphs, -1).to(device)\n",
        "        loss = criterion(output, target)\n",
        "        test_loss += loss.item() * data.num_graphs\n",
        "        all_preds.extend(output.cpu().numpy())\n",
        "        all_targets.extend(target.cpu().numpy())\n",
        "test_loss /= len(test_loader.dataset)\n",
        "test_rmse = test_loss ** 0.5\n",
        "\n",
        "# Compute R^2\n",
        "all_preds = np.array(all_preds).flatten()\n",
        "all_targets = np.array(all_targets).flatten()\n",
        "r2 = r2_score(all_targets, all_preds)\n",
        "\n",
        "# Compute 95% Confidence Interval for RMSE\n",
        "confidence = 0.95\n",
        "squared_errors = (all_preds - all_targets) ** 2\n",
        "mean_se = np.mean(squared_errors)\n",
        "se = stats.sem(squared_errors)\n",
        "interval = stats.t.interval(confidence, len(squared_errors)-1, loc=mean_se, scale=se)\n",
        "ci_lower, ci_upper = np.sqrt(interval[0]), np.sqrt(interval[1])\n",
        "\n",
        "print(f\"Test RMSE: {test_rmse:.4f}, R²: {r2:.4f}, CI (95%): [{ci_lower:.4f}, {ci_upper:.4f}]\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model perfoms a decent bit better!"
      ],
      "metadata": {
        "id": "bR2BRenHrWFs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementing dropout (0.35) to help with generalization. Also introducing LayerNorm based on proff's advice. Changes are in gcn_change1"
      ],
      "metadata": {
        "id": "GAiMBEJKr4EH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gcn_change1 import GCN\n",
        "# Set seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "num_node = train_graph_list[0].x.shape[1]\n",
        "edge_attr = train_graph_list[0].edge_attr.shape[1]\n",
        "u_d = train_graph_list[0].u.shape[1]\n",
        "\n",
        "model = GCN(num_node_features=num_node,\n",
        "            edge_attr_dim=edge_attr,\n",
        "            u_dim=u_d,\n",
        "            hidden_dim=64,\n",
        "            output_dim=1).to(device)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay= 0.01) # introduced AdamW\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-6) # introduced Cosine Annealign Scheduler\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 50\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for data in train_loader:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        target = data.y.view(data.num_graphs, -1).to(device)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * data.num_graphs\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "\n",
        "    # Validation step\n",
        "    model.eval()\n",
        "    all_preds, all_targets = [], []\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for data in val_loader:\n",
        "            data = data.to(device)\n",
        "            output = model(data)\n",
        "            target = data.y.view(data.num_graphs, -1).to(device)\n",
        "            loss = criterion(output, target) #get loss based on criterion\n",
        "            val_loss += loss.item() * data.num_graphs\n",
        "            all_preds.extend(output.cpu().numpy())\n",
        "            all_targets.extend(target.cpu().numpy())\n",
        "    val_loss /= len(val_loader.dataset) #compute validation loss\n",
        "    val_rmse = val_loss ** 0.5\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    # Compute R^2\n",
        "    all_preds = np.array(all_preds).flatten()\n",
        "    all_targets = np.array(all_targets).flatten()\n",
        "    r2 = r2_score(all_targets, all_preds)\n",
        "\n",
        "    # Compute 95% Confidence Interval for RMSE\n",
        "    confidence = 0.95\n",
        "    squared_errors = (all_preds - all_targets) ** 2\n",
        "    mean_se = np.mean(squared_errors)\n",
        "    se = stats.sem(squared_errors)\n",
        "    interval = stats.t.interval(confidence, len(squared_errors)-1, loc=mean_se, scale=se)\n",
        "    ci_lower, ci_upper = np.sqrt(interval[0]), np.sqrt(interval[1])\n",
        "\n",
        "    print(f\"Epoch: {epoch}, Train Loss: {train_loss:.4f}, Val RMSE: {val_rmse:.4f}, R²: {r2:.4f}, CI (95%):[{ci_lower:.4f}, {ci_upper:.4f}]\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzKGI33UtSLm",
        "outputId": "253ec327-9586-4e97-dd3d-2fcd870c5801"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Train Loss: 341.2259, Val RMSE: 4.1277, R²: -18.8833, CI (95%):[3.8185, 4.4153]\n",
            "Epoch: 2, Train Loss: 37.2352, Val RMSE: 3.3622, R²: -12.1921, CI (95%):[3.1327, 3.5769]\n",
            "Epoch: 3, Train Loss: 19.5527, Val RMSE: 2.6306, R²: -7.0758, CI (95%):[2.4363, 2.8115]\n",
            "Epoch: 4, Train Loss: 11.6486, Val RMSE: 2.4061, R²: -5.7564, CI (95%):[2.2653, 2.5391]\n",
            "Epoch: 5, Train Loss: 6.5833, Val RMSE: 1.5856, R²: -1.9342, CI (95%):[1.4722, 1.6915]\n",
            "Epoch: 6, Train Loss: 3.6747, Val RMSE: 1.1461, R²: -0.5330, CI (95%):[1.0583, 1.2277]\n",
            "Epoch: 7, Train Loss: 2.0262, Val RMSE: 0.9361, R²: -0.0227, CI (95%):[0.8791, 0.9899]\n",
            "Epoch: 8, Train Loss: 1.2075, Val RMSE: 0.7886, R²: 0.2743, CI (95%):[0.7293, 0.8437]\n",
            "Epoch: 9, Train Loss: 0.9191, Val RMSE: 0.6908, R²: 0.4431, CI (95%):[0.6473, 0.7318]\n",
            "Epoch: 10, Train Loss: 0.7575, Val RMSE: 0.6646, R²: 0.4845, CI (95%):[0.6271, 0.7000]\n",
            "Epoch: 11, Train Loss: 0.6960, Val RMSE: 0.6628, R²: 0.4873, CI (95%):[0.6307, 0.6934]\n",
            "Epoch: 12, Train Loss: 0.6452, Val RMSE: 0.6308, R²: 0.5356, CI (95%):[0.5996, 0.6605]\n",
            "Epoch: 13, Train Loss: 0.5891, Val RMSE: 0.5923, R²: 0.5906, CI (95%):[0.5627, 0.6205]\n",
            "Epoch: 14, Train Loss: 0.5572, Val RMSE: 0.5877, R²: 0.5969, CI (95%):[0.5589, 0.6152]\n",
            "Epoch: 15, Train Loss: 0.5153, Val RMSE: 0.5653, R²: 0.6270, CI (95%):[0.5361, 0.5932]\n",
            "Epoch: 16, Train Loss: 0.5064, Val RMSE: 0.5721, R²: 0.6180, CI (95%):[0.5447, 0.5983]\n",
            "Epoch: 17, Train Loss: 0.4909, Val RMSE: 0.5924, R²: 0.5905, CI (95%):[0.5650, 0.6186]\n",
            "Epoch: 18, Train Loss: 0.5070, Val RMSE: 0.5814, R²: 0.6055, CI (95%):[0.5533, 0.6082]\n",
            "Epoch: 19, Train Loss: 0.4645, Val RMSE: 0.5720, R²: 0.6182, CI (95%):[0.5439, 0.5987]\n",
            "Epoch: 20, Train Loss: 0.4520, Val RMSE: 0.5414, R²: 0.6579, CI (95%):[0.5125, 0.5689]\n",
            "Epoch: 21, Train Loss: 0.4486, Val RMSE: 0.5635, R²: 0.6295, CI (95%):[0.5353, 0.5904]\n",
            "Epoch: 22, Train Loss: 0.4303, Val RMSE: 0.5924, R²: 0.5905, CI (95%):[0.5620, 0.6213]\n",
            "Epoch: 23, Train Loss: 0.4260, Val RMSE: 0.5619, R²: 0.6315, CI (95%):[0.5337, 0.5888]\n",
            "Epoch: 24, Train Loss: 0.4149, Val RMSE: 0.5525, R²: 0.6437, CI (95%):[0.5247, 0.5790]\n",
            "Epoch: 25, Train Loss: 0.4051, Val RMSE: 0.5729, R²: 0.6170, CI (95%):[0.5456, 0.5990]\n",
            "Epoch: 26, Train Loss: 0.4066, Val RMSE: 0.5676, R²: 0.6240, CI (95%):[0.5372, 0.5965]\n",
            "Epoch: 27, Train Loss: 0.3947, Val RMSE: 0.5574, R²: 0.6374, CI (95%):[0.5294, 0.5841]\n",
            "Epoch: 28, Train Loss: 0.3901, Val RMSE: 0.5858, R²: 0.5995, CI (95%):[0.5564, 0.6138]\n",
            "Epoch: 29, Train Loss: 0.3848, Val RMSE: 0.5971, R²: 0.5840, CI (95%):[0.5685, 0.6243]\n",
            "Epoch: 30, Train Loss: 0.3728, Val RMSE: 0.5487, R²: 0.6486, CI (95%):[0.5200, 0.5761]\n",
            "Epoch: 31, Train Loss: 0.3780, Val RMSE: 0.5475, R²: 0.6502, CI (95%):[0.5180, 0.5755]\n",
            "Epoch: 32, Train Loss: 0.3625, Val RMSE: 0.5382, R²: 0.6620, CI (95%):[0.5099, 0.5651]\n",
            "Epoch: 33, Train Loss: 0.3582, Val RMSE: 0.5490, R²: 0.6482, CI (95%):[0.5190, 0.5775]\n",
            "Epoch: 34, Train Loss: 0.3571, Val RMSE: 0.5426, R²: 0.6564, CI (95%):[0.5128, 0.5709]\n",
            "Epoch: 35, Train Loss: 0.3547, Val RMSE: 0.5375, R²: 0.6628, CI (95%):[0.5088, 0.5648]\n",
            "Epoch: 36, Train Loss: 0.3611, Val RMSE: 0.5409, R²: 0.6586, CI (95%):[0.5115, 0.5688]\n",
            "Epoch: 37, Train Loss: 0.3498, Val RMSE: 0.5344, R²: 0.6667, CI (95%):[0.5050, 0.5622]\n",
            "Epoch: 38, Train Loss: 0.3378, Val RMSE: 0.5377, R²: 0.6625, CI (95%):[0.5070, 0.5668]\n",
            "Epoch: 39, Train Loss: 0.3349, Val RMSE: 0.5438, R²: 0.6549, CI (95%):[0.5133, 0.5726]\n",
            "Epoch: 40, Train Loss: 0.3330, Val RMSE: 0.5388, R²: 0.6612, CI (95%):[0.5089, 0.5672]\n",
            "Epoch: 41, Train Loss: 0.3283, Val RMSE: 0.5477, R²: 0.6499, CI (95%):[0.5172, 0.5766]\n",
            "Epoch: 42, Train Loss: 0.3267, Val RMSE: 0.5631, R²: 0.6299, CI (95%):[0.5317, 0.5929]\n",
            "Epoch: 43, Train Loss: 0.3277, Val RMSE: 0.5580, R²: 0.6366, CI (95%):[0.5268, 0.5876]\n",
            "Epoch: 44, Train Loss: 0.3234, Val RMSE: 0.5419, R²: 0.6573, CI (95%):[0.5106, 0.5715]\n",
            "Epoch: 45, Train Loss: 0.3239, Val RMSE: 0.5378, R²: 0.6624, CI (95%):[0.5075, 0.5665]\n",
            "Epoch: 46, Train Loss: 0.3194, Val RMSE: 0.5358, R²: 0.6650, CI (95%):[0.5063, 0.5638]\n",
            "Epoch: 47, Train Loss: 0.3182, Val RMSE: 0.5336, R²: 0.6677, CI (95%):[0.5042, 0.5614]\n",
            "Epoch: 48, Train Loss: 0.3155, Val RMSE: 0.5365, R²: 0.6641, CI (95%):[0.5066, 0.5647]\n",
            "Epoch: 49, Train Loss: 0.3172, Val RMSE: 0.5361, R²: 0.6646, CI (95%):[0.5062, 0.5645]\n",
            "Epoch: 50, Train Loss: 0.3137, Val RMSE: 0.5372, R²: 0.6633, CI (95%):[0.5073, 0.5655]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "model.eval()\n",
        "test_loss = 0\n",
        "all_preds, all_targets = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        data = data.to(device)\n",
        "        output = model(data)\n",
        "        target = data.y.view(data.num_graphs, -1).to(device)\n",
        "        loss = criterion(output, target)\n",
        "        test_loss += loss.item() * data.num_graphs\n",
        "        all_preds.extend(output.cpu().numpy())\n",
        "        all_targets.extend(target.cpu().numpy())\n",
        "test_loss /= len(test_loader.dataset)\n",
        "test_rmse = test_loss ** 0.5\n",
        "\n",
        "# Compute R^2\n",
        "all_preds = np.array(all_preds).flatten()\n",
        "all_targets = np.array(all_targets).flatten()\n",
        "r2 = r2_score(all_targets, all_preds)\n",
        "\n",
        "# Compute 95% Confidence Interval for RMSE\n",
        "confidence = 0.95\n",
        "squared_errors = (all_preds - all_targets) ** 2\n",
        "mean_se = np.mean(squared_errors)\n",
        "se = stats.sem(squared_errors)\n",
        "interval = stats.t.interval(confidence, len(squared_errors)-1, loc=mean_se, scale=se)\n",
        "ci_lower, ci_upper = np.sqrt(interval[0]), np.sqrt(interval[1])\n",
        "\n",
        "print(f\"Test RMSE: {test_rmse:.4f}, R²: {r2:.4f}, CI (95%): [{ci_lower:.4f}, {ci_upper:.4f}]\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4omPdfyivETs",
        "outputId": "6c820d43-a601-40f5-ca19-ae83fa7b4352"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test RMSE: 0.4938, R²: 0.6340, CI (95%): [0.4690, 0.5174]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performance is roughly the same. No harm in keeping them then!"
      ],
      "metadata": {
        "id": "jSG53vIYvJMI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, going to try different pooling methods. Mean pooling is quite basic.\n",
        "First, trying add pool (since many molecular features are additive generally). Changes in gcn_change2"
      ],
      "metadata": {
        "id": "8PXqmqTrwX23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gcn_change2 import GCN\n",
        "# Set seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "num_node = train_graph_list[0].x.shape[1]\n",
        "edge_attr = train_graph_list[0].edge_attr.shape[1]\n",
        "u_d = train_graph_list[0].u.shape[1]\n",
        "\n",
        "model = GCN(num_node_features=num_node,\n",
        "            edge_attr_dim=edge_attr,\n",
        "            u_dim=u_d,\n",
        "            hidden_dim=64,\n",
        "            output_dim=1).to(device)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay= 0.01) # introduced AdamW\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-6) # introduced Cosine Annealign Scheduler\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 50\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for data in train_loader:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        target = data.y.view(data.num_graphs, -1).to(device)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * data.num_graphs\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "\n",
        "    # Validation step\n",
        "    model.eval()\n",
        "    all_preds, all_targets = [], []\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for data in val_loader:\n",
        "            data = data.to(device)\n",
        "            output = model(data)\n",
        "            target = data.y.view(data.num_graphs, -1).to(device)\n",
        "            loss = criterion(output, target) #get loss based on criterion\n",
        "            val_loss += loss.item() * data.num_graphs\n",
        "            all_preds.extend(output.cpu().numpy())\n",
        "            all_targets.extend(target.cpu().numpy())\n",
        "    val_loss /= len(val_loader.dataset) #compute validation loss\n",
        "    val_rmse = val_loss ** 0.5\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    # Compute R^2\n",
        "    all_preds = np.array(all_preds).flatten()\n",
        "    all_targets = np.array(all_targets).flatten()\n",
        "    r2 = r2_score(all_targets, all_preds)\n",
        "\n",
        "    # Compute 95% Confidence Interval for RMSE\n",
        "    confidence = 0.95\n",
        "    squared_errors = (all_preds - all_targets) ** 2\n",
        "    mean_se = np.mean(squared_errors)\n",
        "    se = stats.sem(squared_errors)\n",
        "    interval = stats.t.interval(confidence, len(squared_errors)-1, loc=mean_se, scale=se)\n",
        "    ci_lower, ci_upper = np.sqrt(interval[0]), np.sqrt(interval[1])\n",
        "\n",
        "    print(f\"Epoch: {epoch}, Train Loss: {train_loss:.4f}, Val RMSE: {val_rmse:.4f}, R²: {r2:.4f}, CI (95%):[{ci_lower:.4f}, {ci_upper:.4f}]\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZjary3JwfAn",
        "outputId": "59928bc6-bae1-4dad-bdbd-e1b9c5bf8d53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Train Loss: 226.9515, Val RMSE: 4.7634, R²: -25.4796, CI (95%):[4.5957, 4.9254]\n",
            "Epoch: 2, Train Loss: 50.0830, Val RMSE: 3.2526, R²: -11.3462, CI (95%):[3.0800, 3.4165]\n",
            "Epoch: 3, Train Loss: 23.5278, Val RMSE: 2.2275, R²: -4.7904, CI (95%):[2.1062, 2.3425]\n",
            "Epoch: 4, Train Loss: 10.4504, Val RMSE: 1.8344, R²: -2.9272, CI (95%):[1.7423, 1.9222]\n",
            "Epoch: 5, Train Loss: 5.3104, Val RMSE: 1.1661, R²: -0.5870, CI (95%):[1.0943, 1.2338]\n",
            "Epoch: 6, Train Loss: 3.0357, Val RMSE: 1.0770, R²: -0.3536, CI (95%):[1.0244, 1.1270]\n",
            "Epoch: 7, Train Loss: 2.2403, Val RMSE: 0.9267, R²: -0.0021, CI (95%):[0.8794, 0.9716]\n",
            "Epoch: 8, Train Loss: 1.6545, Val RMSE: 0.8422, R²: 0.1722, CI (95%):[0.8005, 0.8819]\n",
            "Epoch: 9, Train Loss: 1.4914, Val RMSE: 0.8049, R²: 0.2440, CI (95%):[0.7608, 0.8467]\n",
            "Epoch: 10, Train Loss: 1.2161, Val RMSE: 0.8531, R²: 0.1506, CI (95%):[0.8146, 0.8900]\n",
            "Epoch: 11, Train Loss: 1.1673, Val RMSE: 0.8686, R²: 0.1195, CI (95%):[0.8308, 0.9049]\n",
            "Epoch: 12, Train Loss: 1.0342, Val RMSE: 0.7483, R²: 0.3466, CI (95%):[0.7135, 0.7815]\n",
            "Epoch: 13, Train Loss: 1.0784, Val RMSE: 0.7081, R²: 0.4149, CI (95%):[0.6729, 0.7416]\n",
            "Epoch: 14, Train Loss: 0.8797, Val RMSE: 0.6905, R²: 0.4436, CI (95%):[0.6549, 0.7244]\n",
            "Epoch: 15, Train Loss: 0.8489, Val RMSE: 0.6841, R²: 0.4539, CI (95%):[0.6506, 0.7160]\n",
            "Epoch: 16, Train Loss: 0.8054, Val RMSE: 0.8979, R²: 0.0592, CI (95%):[0.8586, 0.9355]\n",
            "Epoch: 17, Train Loss: 0.8091, Val RMSE: 0.6937, R²: 0.4384, CI (95%):[0.6581, 0.7276]\n",
            "Epoch: 18, Train Loss: 0.8087, Val RMSE: 0.8702, R²: 0.1163, CI (95%):[0.8304, 0.9082]\n",
            "Epoch: 19, Train Loss: 0.8241, Val RMSE: 0.6615, R²: 0.4893, CI (95%):[0.6299, 0.6917]\n",
            "Epoch: 20, Train Loss: 0.9427, Val RMSE: 0.7054, R²: 0.4193, CI (95%):[0.6702, 0.7389]\n",
            "Epoch: 21, Train Loss: 0.8572, Val RMSE: 0.6985, R²: 0.4305, CI (95%):[0.6668, 0.7289]\n",
            "Epoch: 22, Train Loss: 0.7225, Val RMSE: 0.6831, R²: 0.4555, CI (95%):[0.6508, 0.7139]\n",
            "Epoch: 23, Train Loss: 0.7400, Val RMSE: 0.6764, R²: 0.4661, CI (95%):[0.6450, 0.7064]\n",
            "Epoch: 24, Train Loss: 0.6714, Val RMSE: 0.6669, R²: 0.4809, CI (95%):[0.6359, 0.6966]\n",
            "Epoch: 25, Train Loss: 0.7730, Val RMSE: 0.6649, R²: 0.4841, CI (95%):[0.6334, 0.6949]\n",
            "Epoch: 26, Train Loss: 0.7479, Val RMSE: 0.6774, R²: 0.4645, CI (95%):[0.6444, 0.7089]\n",
            "Epoch: 27, Train Loss: 0.6961, Val RMSE: 0.6911, R²: 0.4425, CI (95%):[0.6586, 0.7222]\n",
            "Epoch: 28, Train Loss: 0.6446, Val RMSE: 0.6592, R²: 0.4928, CI (95%):[0.6254, 0.6914]\n",
            "Epoch: 29, Train Loss: 0.7114, Val RMSE: 0.6560, R²: 0.4978, CI (95%):[0.6237, 0.6868]\n",
            "Epoch: 30, Train Loss: 0.7478, Val RMSE: 0.6761, R²: 0.4666, CI (95%):[0.6424, 0.7081]\n",
            "Epoch: 31, Train Loss: 0.6419, Val RMSE: 0.7090, R²: 0.4134, CI (95%):[0.6743, 0.7421]\n",
            "Epoch: 32, Train Loss: 0.6655, Val RMSE: 0.7245, R²: 0.3874, CI (95%):[0.6927, 0.7549]\n",
            "Epoch: 33, Train Loss: 0.7034, Val RMSE: 0.6654, R²: 0.4833, CI (95%):[0.6322, 0.6970]\n",
            "Epoch: 34, Train Loss: 0.6082, Val RMSE: 0.6737, R²: 0.4703, CI (95%):[0.6394, 0.7064]\n",
            "Epoch: 35, Train Loss: 0.6362, Val RMSE: 0.6808, R²: 0.4591, CI (95%):[0.6474, 0.7127]\n",
            "Epoch: 36, Train Loss: 0.6180, Val RMSE: 0.6714, R²: 0.4740, CI (95%):[0.6382, 0.7030]\n",
            "Epoch: 37, Train Loss: 0.6016, Val RMSE: 0.6634, R²: 0.4864, CI (95%):[0.6310, 0.6944]\n",
            "Epoch: 38, Train Loss: 0.6197, Val RMSE: 0.6691, R²: 0.4775, CI (95%):[0.6364, 0.7004]\n",
            "Epoch: 39, Train Loss: 0.5589, Val RMSE: 0.6485, R²: 0.5092, CI (95%):[0.6159, 0.6796]\n",
            "Epoch: 40, Train Loss: 0.5873, Val RMSE: 0.7362, R²: 0.3675, CI (95%):[0.7006, 0.7701]\n",
            "Epoch: 41, Train Loss: 0.5935, Val RMSE: 0.6801, R²: 0.4603, CI (95%):[0.6470, 0.7116]\n",
            "Epoch: 42, Train Loss: 0.5535, Val RMSE: 0.6727, R²: 0.4718, CI (95%):[0.6389, 0.7050]\n",
            "Epoch: 43, Train Loss: 0.5527, Val RMSE: 0.6721, R²: 0.4729, CI (95%):[0.6386, 0.7039]\n",
            "Epoch: 44, Train Loss: 0.5696, Val RMSE: 0.6945, R²: 0.4371, CI (95%):[0.6594, 0.7280]\n",
            "Epoch: 45, Train Loss: 0.5444, Val RMSE: 0.6562, R²: 0.4975, CI (95%):[0.6236, 0.6872]\n",
            "Epoch: 46, Train Loss: 0.5353, Val RMSE: 0.6502, R²: 0.5067, CI (95%):[0.6181, 0.6807]\n",
            "Epoch: 47, Train Loss: 0.5404, Val RMSE: 0.6556, R²: 0.4984, CI (95%):[0.6237, 0.6861]\n",
            "Epoch: 48, Train Loss: 0.5362, Val RMSE: 0.6553, R²: 0.4988, CI (95%):[0.6230, 0.6861]\n",
            "Epoch: 49, Train Loss: 0.5396, Val RMSE: 0.6551, R²: 0.4992, CI (95%):[0.6231, 0.6856]\n",
            "Epoch: 50, Train Loss: 0.5361, Val RMSE: 0.6563, R²: 0.4973, CI (95%):[0.6239, 0.6872]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "No, this one performed much worse. Let's try a more complex method, set2set from torch_geometric. Changes in gcn_change3"
      ],
      "metadata": {
        "id": "ZZbs9woxyOsp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gcn_change3 import GCN\n",
        "# Set seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "num_node = train_graph_list[0].x.shape[1]\n",
        "edge_attr = train_graph_list[0].edge_attr.shape[1]\n",
        "u_d = train_graph_list[0].u.shape[1]\n",
        "\n",
        "model = GCN(num_node_features=num_node,\n",
        "            edge_attr_dim=edge_attr,\n",
        "            u_dim=u_d,\n",
        "            hidden_dim=64,\n",
        "            output_dim=1).to(device)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay= 0.01) # introduced AdamW\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-6) # introduced Cosine Annealign Scheduler\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 50\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for data in train_loader:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        target = data.y.view(data.num_graphs, -1).to(device)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * data.num_graphs\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "\n",
        "    # Validation step\n",
        "    model.eval()\n",
        "    all_preds, all_targets = [], []\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for data in val_loader:\n",
        "            data = data.to(device)\n",
        "            output = model(data)\n",
        "            target = data.y.view(data.num_graphs, -1).to(device)\n",
        "            loss = criterion(output, target) #get loss based on criterion\n",
        "            val_loss += loss.item() * data.num_graphs\n",
        "            all_preds.extend(output.cpu().numpy())\n",
        "            all_targets.extend(target.cpu().numpy())\n",
        "    val_loss /= len(val_loader.dataset) #compute validation loss\n",
        "    val_rmse = val_loss ** 0.5\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    # Compute R^2\n",
        "    all_preds = np.array(all_preds).flatten()\n",
        "    all_targets = np.array(all_targets).flatten()\n",
        "    r2 = r2_score(all_targets, all_preds)\n",
        "\n",
        "    # Compute 95% Confidence Interval for RMSE\n",
        "    confidence = 0.95\n",
        "    squared_errors = (all_preds - all_targets) ** 2\n",
        "    mean_se = np.mean(squared_errors)\n",
        "    se = stats.sem(squared_errors)\n",
        "    interval = stats.t.interval(confidence, len(squared_errors)-1, loc=mean_se, scale=se)\n",
        "    ci_lower, ci_upper = np.sqrt(interval[0]), np.sqrt(interval[1])\n",
        "\n",
        "    print(f\"Epoch: {epoch}, Train Loss: {train_loss:.4f}, Val RMSE: {val_rmse:.4f}, R²: {r2:.4f}, CI (95%):[{ci_lower:.4f}, {ci_upper:.4f}]\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LS4h-n6O0GKo",
        "outputId": "f0900e88-e306-4cde-eb19-06254c18c76d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Train Loss: 516.4796, Val RMSE: 4.2576, R²: -20.1545, CI (95%):[3.8954, 4.5913]\n",
            "Epoch: 2, Train Loss: 19.3141, Val RMSE: 3.1152, R²: -10.3251, CI (95%):[2.8519, 3.3579]\n",
            "Epoch: 3, Train Loss: 10.5381, Val RMSE: 2.2250, R²: -4.7774, CI (95%):[2.0504, 2.3869]\n",
            "Epoch: 4, Train Loss: 6.3784, Val RMSE: 1.7072, R²: -2.4012, CI (95%):[1.5895, 1.8172]\n",
            "Epoch: 5, Train Loss: 4.0824, Val RMSE: 1.2648, R²: -0.8668, CI (95%):[1.1765, 1.3473]\n",
            "Epoch: 6, Train Loss: 2.9577, Val RMSE: 1.0395, R²: -0.2611, CI (95%):[0.9807, 1.0953]\n",
            "Epoch: 7, Train Loss: 2.3172, Val RMSE: 0.9541, R²: -0.0624, CI (95%):[0.9000, 1.0053]\n",
            "Epoch: 8, Train Loss: 1.9219, Val RMSE: 0.8058, R²: 0.2422, CI (95%):[0.7564, 0.8523]\n",
            "Epoch: 9, Train Loss: 1.6122, Val RMSE: 0.8201, R²: 0.2150, CI (95%):[0.7754, 0.8625]\n",
            "Epoch: 10, Train Loss: 1.4029, Val RMSE: 0.7626, R²: 0.3212, CI (95%):[0.7211, 0.8020]\n",
            "Epoch: 11, Train Loss: 1.1904, Val RMSE: 0.8269, R²: 0.2020, CI (95%):[0.7875, 0.8645]\n",
            "Epoch: 12, Train Loss: 1.0097, Val RMSE: 0.8013, R²: 0.2506, CI (95%):[0.7620, 0.8388]\n",
            "Epoch: 13, Train Loss: 0.9229, Val RMSE: 0.7420, R²: 0.3575, CI (95%):[0.7042, 0.7780]\n",
            "Epoch: 14, Train Loss: 0.8011, Val RMSE: 0.6620, R²: 0.4885, CI (95%):[0.6299, 0.6927]\n",
            "Epoch: 15, Train Loss: 0.6983, Val RMSE: 0.7969, R²: 0.2589, CI (95%):[0.7580, 0.8339]\n",
            "Epoch: 16, Train Loss: 0.6704, Val RMSE: 0.6349, R²: 0.5296, CI (95%):[0.6042, 0.6642]\n",
            "Epoch: 17, Train Loss: 0.6278, Val RMSE: 0.6424, R²: 0.5184, CI (95%):[0.6115, 0.6718]\n",
            "Epoch: 18, Train Loss: 0.5981, Val RMSE: 0.6249, R²: 0.5443, CI (95%):[0.5953, 0.6531]\n",
            "Epoch: 19, Train Loss: 0.5867, Val RMSE: 0.6156, R²: 0.5577, CI (95%):[0.5859, 0.6440]\n",
            "Epoch: 20, Train Loss: 0.5921, Val RMSE: 0.6397, R²: 0.5224, CI (95%):[0.6102, 0.6679]\n",
            "Epoch: 21, Train Loss: 0.5028, Val RMSE: 0.6264, R²: 0.5422, CI (95%):[0.5971, 0.6543]\n",
            "Epoch: 22, Train Loss: 0.5338, Val RMSE: 0.5906, R²: 0.5930, CI (95%):[0.5588, 0.6207]\n",
            "Epoch: 23, Train Loss: 0.5376, Val RMSE: 0.5916, R²: 0.5915, CI (95%):[0.5608, 0.6209]\n",
            "Epoch: 24, Train Loss: 0.5509, Val RMSE: 0.5833, R²: 0.6029, CI (95%):[0.5540, 0.6112]\n",
            "Epoch: 25, Train Loss: 0.4746, Val RMSE: 0.5850, R²: 0.6007, CI (95%):[0.5561, 0.6125]\n",
            "Epoch: 26, Train Loss: 0.4758, Val RMSE: 0.6180, R²: 0.5543, CI (95%):[0.5894, 0.6454]\n",
            "Epoch: 27, Train Loss: 0.4616, Val RMSE: 0.6520, R²: 0.5040, CI (95%):[0.6221, 0.6806]\n",
            "Epoch: 28, Train Loss: 0.4809, Val RMSE: 0.5705, R²: 0.6202, CI (95%):[0.5412, 0.5983]\n",
            "Epoch: 29, Train Loss: 0.4419, Val RMSE: 0.5709, R²: 0.6197, CI (95%):[0.5412, 0.5990]\n",
            "Epoch: 30, Train Loss: 0.5150, Val RMSE: 0.6520, R²: 0.5038, CI (95%):[0.6230, 0.6798]\n",
            "Epoch: 31, Train Loss: 0.4449, Val RMSE: 0.6509, R²: 0.5056, CI (95%):[0.6207, 0.6797]\n",
            "Epoch: 32, Train Loss: 0.4319, Val RMSE: 0.5675, R²: 0.6241, CI (95%):[0.5380, 0.5956]\n",
            "Epoch: 33, Train Loss: 0.4456, Val RMSE: 0.5721, R²: 0.6181, CI (95%):[0.5421, 0.6005]\n",
            "Epoch: 34, Train Loss: 0.4751, Val RMSE: 0.5895, R²: 0.5944, CI (95%):[0.5609, 0.6168]\n",
            "Epoch: 35, Train Loss: 0.4147, Val RMSE: 0.5783, R²: 0.6097, CI (95%):[0.5497, 0.6055]\n",
            "Epoch: 36, Train Loss: 0.4045, Val RMSE: 0.5698, R²: 0.6212, CI (95%):[0.5402, 0.5978]\n",
            "Epoch: 37, Train Loss: 0.3996, Val RMSE: 0.5724, R²: 0.6176, CI (95%):[0.5435, 0.5999]\n",
            "Epoch: 38, Train Loss: 0.3875, Val RMSE: 0.5738, R²: 0.6158, CI (95%):[0.5451, 0.6011]\n",
            "Epoch: 39, Train Loss: 0.3935, Val RMSE: 0.5584, R²: 0.6361, CI (95%):[0.5294, 0.5859]\n",
            "Epoch: 40, Train Loss: 0.4008, Val RMSE: 0.5621, R²: 0.6312, CI (95%):[0.5326, 0.5902]\n",
            "Epoch: 41, Train Loss: 0.3825, Val RMSE: 0.5585, R²: 0.6360, CI (95%):[0.5291, 0.5864]\n",
            "Epoch: 42, Train Loss: 0.3822, Val RMSE: 0.5498, R²: 0.6472, CI (95%):[0.5204, 0.5777]\n",
            "Epoch: 43, Train Loss: 0.3725, Val RMSE: 0.5755, R²: 0.6135, CI (95%):[0.5468, 0.6029]\n",
            "Epoch: 44, Train Loss: 0.3794, Val RMSE: 0.5748, R²: 0.6144, CI (95%):[0.5462, 0.6021]\n",
            "Epoch: 45, Train Loss: 0.3670, Val RMSE: 0.5634, R²: 0.6296, CI (95%):[0.5346, 0.5908]\n",
            "Epoch: 46, Train Loss: 0.3696, Val RMSE: 0.5603, R²: 0.6337, CI (95%):[0.5315, 0.5877]\n",
            "Epoch: 47, Train Loss: 0.3694, Val RMSE: 0.5585, R²: 0.6360, CI (95%):[0.5295, 0.5861]\n",
            "Epoch: 48, Train Loss: 0.3629, Val RMSE: 0.5573, R²: 0.6376, CI (95%):[0.5282, 0.5849]\n",
            "Epoch: 49, Train Loss: 0.3670, Val RMSE: 0.5589, R²: 0.6355, CI (95%):[0.5298, 0.5865]\n",
            "Epoch: 50, Train Loss: 0.3593, Val RMSE: 0.5629, R²: 0.6302, CI (95%):[0.5341, 0.5904]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "model.eval()\n",
        "test_loss = 0\n",
        "all_preds, all_targets = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        data = data.to(device)\n",
        "        output = model(data)\n",
        "        target = data.y.view(data.num_graphs, -1).to(device)\n",
        "        loss = criterion(output, target)\n",
        "        test_loss += loss.item() * data.num_graphs\n",
        "        all_preds.extend(output.cpu().numpy())\n",
        "        all_targets.extend(target.cpu().numpy())\n",
        "test_loss /= len(test_loader.dataset)\n",
        "test_rmse = test_loss ** 0.5\n",
        "\n",
        "# Compute R^2\n",
        "all_preds = np.array(all_preds).flatten()\n",
        "all_targets = np.array(all_targets).flatten()\n",
        "r2 = r2_score(all_targets, all_preds)\n",
        "\n",
        "# Compute 95% Confidence Interval for RMSE\n",
        "confidence = 0.95\n",
        "squared_errors = (all_preds - all_targets) ** 2\n",
        "mean_se = np.mean(squared_errors)\n",
        "se = stats.sem(squared_errors)\n",
        "interval = stats.t.interval(confidence, len(squared_errors)-1, loc=mean_se, scale=se)\n",
        "ci_lower, ci_upper = np.sqrt(interval[0]), np.sqrt(interval[1])\n",
        "\n",
        "print(f\"Test RMSE: {test_rmse:.4f}, R²: {r2:.4f}, CI (95%): [{ci_lower:.4f}, {ci_upper:.4f}]\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27060PiB2KU3",
        "outputId": "598008f2-9c11-4071-bca6-8c4f365a6122"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test RMSE: 0.5397, R²: 0.5629, CI (95%): [0.5107, 0.5671]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This one was also slightly worse, might come back to it later in time. For now, lets just roll with change 1 of including the layer norm and dropout."
      ],
      "metadata": {
        "id": "kIIYnnZa2OnI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One last change I am going to try is just adding another fully connected layer to allow for some more learning wiht the global features."
      ],
      "metadata": {
        "id": "QZ3v7xl13d70"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gcn_change4 import GCN\n",
        "# Set seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "num_node = train_graph_list[0].x.shape[1]\n",
        "edge_attr = train_graph_list[0].edge_attr.shape[1]\n",
        "u_d = train_graph_list[0].u.shape[1]\n",
        "\n",
        "model = GCN(num_node_features=num_node,\n",
        "            edge_attr_dim=edge_attr,\n",
        "            u_dim=u_d,\n",
        "            hidden_dim=64,\n",
        "            output_dim=1).to(device)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay= 0.01) # introduced AdamW\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-6) # introduced Cosine Annealign Scheduler\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 50\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for data in train_loader:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        target = data.y.view(data.num_graphs, -1).to(device)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * data.num_graphs\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "\n",
        "    # Validation step\n",
        "    model.eval()\n",
        "    all_preds, all_targets = [], []\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for data in val_loader:\n",
        "            data = data.to(device)\n",
        "            output = model(data)\n",
        "            target = data.y.view(data.num_graphs, -1).to(device)\n",
        "            loss = criterion(output, target) #get loss based on criterion\n",
        "            val_loss += loss.item() * data.num_graphs\n",
        "            all_preds.extend(output.cpu().numpy())\n",
        "            all_targets.extend(target.cpu().numpy())\n",
        "    val_loss /= len(val_loader.dataset) #compute validation loss\n",
        "    val_rmse = val_loss ** 0.5\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    # Compute R^2\n",
        "    all_preds = np.array(all_preds).flatten()\n",
        "    all_targets = np.array(all_targets).flatten()\n",
        "    r2 = r2_score(all_targets, all_preds)\n",
        "\n",
        "    # Compute 95% Confidence Interval for RMSE\n",
        "    confidence = 0.95\n",
        "    squared_errors = (all_preds - all_targets) ** 2\n",
        "    mean_se = np.mean(squared_errors)\n",
        "    se = stats.sem(squared_errors)\n",
        "    interval = stats.t.interval(confidence, len(squared_errors)-1, loc=mean_se, scale=se)\n",
        "    ci_lower, ci_upper = np.sqrt(interval[0]), np.sqrt(interval[1])\n",
        "\n",
        "    print(f\"Epoch: {epoch}, Train Loss: {train_loss:.4f}, Val RMSE: {val_rmse:.4f}, R²: {r2:.4f}, CI (95%):[{ci_lower:.4f}, {ci_upper:.4f}]\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LdjLLqSQ3jQ5",
        "outputId": "074ae986-fa40-4c59-b7dd-28c8e46ead3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Train Loss: 90.8583, Val RMSE: 1.0416, R²: -0.2660, CI (95%):[0.9052, 1.1620]\n",
            "Epoch: 2, Train Loss: 7.4894, Val RMSE: 0.7955, R²: 0.2615, CI (95%):[0.7568, 0.8324]\n",
            "Epoch: 3, Train Loss: 1.8274, Val RMSE: 0.7011, R²: 0.4263, CI (95%):[0.6670, 0.7336]\n",
            "Epoch: 4, Train Loss: 1.1954, Val RMSE: 0.7213, R²: 0.3929, CI (95%):[0.6876, 0.7535]\n",
            "Epoch: 5, Train Loss: 0.9093, Val RMSE: 0.6845, R²: 0.4532, CI (95%):[0.6502, 0.7171]\n",
            "Epoch: 6, Train Loss: 0.7990, Val RMSE: 0.7235, R²: 0.3891, CI (95%):[0.6910, 0.7546]\n",
            "Epoch: 7, Train Loss: 0.7290, Val RMSE: 0.6741, R²: 0.4696, CI (95%):[0.6425, 0.7044]\n",
            "Epoch: 8, Train Loss: 0.6761, Val RMSE: 0.6970, R²: 0.4331, CI (95%):[0.6655, 0.7271]\n",
            "Epoch: 9, Train Loss: 0.6662, Val RMSE: 0.6847, R²: 0.4528, CI (95%):[0.6531, 0.7150]\n",
            "Epoch: 10, Train Loss: 0.6250, Val RMSE: 0.6652, R²: 0.4836, CI (95%):[0.6346, 0.6944]\n",
            "Epoch: 11, Train Loss: 0.6079, Val RMSE: 0.6398, R²: 0.5223, CI (95%):[0.6090, 0.6692]\n",
            "Epoch: 12, Train Loss: 0.5917, Val RMSE: 0.6511, R²: 0.5053, CI (95%):[0.6203, 0.6805]\n",
            "Epoch: 13, Train Loss: 0.5845, Val RMSE: 0.6413, R²: 0.5200, CI (95%):[0.6110, 0.6702]\n",
            "Epoch: 14, Train Loss: 0.5756, Val RMSE: 0.6295, R²: 0.5376, CI (95%):[0.5993, 0.6583]\n",
            "Epoch: 15, Train Loss: 0.5723, Val RMSE: 0.6140, R²: 0.5600, CI (95%):[0.5852, 0.6415]\n",
            "Epoch: 16, Train Loss: 0.5603, Val RMSE: 0.6282, R²: 0.5395, CI (95%):[0.5977, 0.6572]\n",
            "Epoch: 17, Train Loss: 0.5537, Val RMSE: 0.6153, R²: 0.5581, CI (95%):[0.5860, 0.6433]\n",
            "Epoch: 18, Train Loss: 0.5431, Val RMSE: 0.6260, R²: 0.5426, CI (95%):[0.5954, 0.6552]\n",
            "Epoch: 19, Train Loss: 0.5369, Val RMSE: 0.6089, R²: 0.5674, CI (95%):[0.5790, 0.6374]\n",
            "Epoch: 20, Train Loss: 0.5303, Val RMSE: 0.6192, R²: 0.5525, CI (95%):[0.5883, 0.6487]\n",
            "Epoch: 21, Train Loss: 0.5320, Val RMSE: 0.6133, R²: 0.5610, CI (95%):[0.5823, 0.6429]\n",
            "Epoch: 22, Train Loss: 0.5070, Val RMSE: 0.5912, R²: 0.5921, CI (95%):[0.5633, 0.6179]\n",
            "Epoch: 23, Train Loss: 0.5052, Val RMSE: 0.5668, R²: 0.6251, CI (95%):[0.5376, 0.5945]\n",
            "Epoch: 24, Train Loss: 0.4992, Val RMSE: 0.5940, R²: 0.5883, CI (95%):[0.5631, 0.6233]\n",
            "Epoch: 25, Train Loss: 0.4779, Val RMSE: 0.5484, R²: 0.6490, CI (95%):[0.5190, 0.5763]\n",
            "Epoch: 26, Train Loss: 0.4903, Val RMSE: 0.5765, R²: 0.6121, CI (95%):[0.5463, 0.6052]\n",
            "Epoch: 27, Train Loss: 0.4766, Val RMSE: 0.5465, R²: 0.6514, CI (95%):[0.5168, 0.5747]\n",
            "Epoch: 28, Train Loss: 0.4745, Val RMSE: 0.5566, R²: 0.6385, CI (95%):[0.5267, 0.5849]\n",
            "Epoch: 29, Train Loss: 0.4699, Val RMSE: 0.5498, R²: 0.6472, CI (95%):[0.5194, 0.5787]\n",
            "Epoch: 30, Train Loss: 0.4723, Val RMSE: 0.5286, R²: 0.6739, CI (95%):[0.4997, 0.5559]\n",
            "Epoch: 31, Train Loss: 0.4472, Val RMSE: 0.5670, R²: 0.6248, CI (95%):[0.5370, 0.5955]\n",
            "Epoch: 32, Train Loss: 0.4567, Val RMSE: 0.5562, R²: 0.6390, CI (95%):[0.5268, 0.5841]\n",
            "Epoch: 33, Train Loss: 0.4572, Val RMSE: 0.5343, R²: 0.6668, CI (95%):[0.5050, 0.5621]\n",
            "Epoch: 34, Train Loss: 0.4458, Val RMSE: 0.5515, R²: 0.6450, CI (95%):[0.5214, 0.5801]\n",
            "Epoch: 35, Train Loss: 0.4346, Val RMSE: 0.5250, R²: 0.6783, CI (95%):[0.4949, 0.5535]\n",
            "Epoch: 36, Train Loss: 0.4449, Val RMSE: 0.5361, R²: 0.6646, CI (95%):[0.5075, 0.5632]\n",
            "Epoch: 37, Train Loss: 0.4267, Val RMSE: 0.5216, R²: 0.6825, CI (95%):[0.4919, 0.5497]\n",
            "Epoch: 38, Train Loss: 0.4240, Val RMSE: 0.5324, R²: 0.6692, CI (95%):[0.5024, 0.5609]\n",
            "Epoch: 39, Train Loss: 0.4236, Val RMSE: 0.5258, R²: 0.6774, CI (95%):[0.4965, 0.5534]\n",
            "Epoch: 40, Train Loss: 0.4236, Val RMSE: 0.5131, R²: 0.6927, CI (95%):[0.4840, 0.5407]\n",
            "Epoch: 41, Train Loss: 0.4171, Val RMSE: 0.5346, R²: 0.6664, CI (95%):[0.5051, 0.5627]\n",
            "Epoch: 42, Train Loss: 0.4155, Val RMSE: 0.5289, R²: 0.6735, CI (95%):[0.4992, 0.5570]\n",
            "Epoch: 43, Train Loss: 0.4152, Val RMSE: 0.5237, R²: 0.6800, CI (95%):[0.4943, 0.5515]\n",
            "Epoch: 44, Train Loss: 0.4137, Val RMSE: 0.5157, R²: 0.6896, CI (95%):[0.4860, 0.5439]\n",
            "Epoch: 45, Train Loss: 0.4156, Val RMSE: 0.5212, R²: 0.6829, CI (95%):[0.4917, 0.5492]\n",
            "Epoch: 46, Train Loss: 0.4123, Val RMSE: 0.5213, R²: 0.6828, CI (95%):[0.4915, 0.5495]\n",
            "Epoch: 47, Train Loss: 0.4092, Val RMSE: 0.5230, R²: 0.6808, CI (95%):[0.4932, 0.5512]\n",
            "Epoch: 48, Train Loss: 0.4183, Val RMSE: 0.5223, R²: 0.6817, CI (95%):[0.4925, 0.5505]\n",
            "Epoch: 49, Train Loss: 0.4128, Val RMSE: 0.5230, R²: 0.6808, CI (95%):[0.4932, 0.5513]\n",
            "Epoch: 50, Train Loss: 0.4078, Val RMSE: 0.5217, R²: 0.6824, CI (95%):[0.4919, 0.5499]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "model.eval()\n",
        "test_loss = 0\n",
        "all_preds, all_targets = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        data = data.to(device)\n",
        "        output = model(data)\n",
        "        target = data.y.view(data.num_graphs, -1).to(device)\n",
        "        loss = criterion(output, target)\n",
        "        test_loss += loss.item() * data.num_graphs\n",
        "        all_preds.extend(output.cpu().numpy())\n",
        "        all_targets.extend(target.cpu().numpy())\n",
        "test_loss /= len(test_loader.dataset)\n",
        "test_rmse = test_loss ** 0.5\n",
        "\n",
        "# Compute R^2\n",
        "all_preds = np.array(all_preds).flatten()\n",
        "all_targets = np.array(all_targets).flatten()\n",
        "r2 = r2_score(all_targets, all_preds)\n",
        "\n",
        "# Compute 95% Confidence Interval for RMSE\n",
        "confidence = 0.95\n",
        "squared_errors = (all_preds - all_targets) ** 2\n",
        "mean_se = np.mean(squared_errors)\n",
        "se = stats.sem(squared_errors)\n",
        "interval = stats.t.interval(confidence, len(squared_errors)-1, loc=mean_se, scale=se)\n",
        "ci_lower, ci_upper = np.sqrt(interval[0]), np.sqrt(interval[1])\n",
        "\n",
        "print(f\"Test RMSE: {test_rmse:.4f}, R²: {r2:.4f}, CI (95%): [{ci_lower:.4f}, {ci_upper:.4f}]\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JoBBNY0X5f8m",
        "outputId": "53c09500-7986-4824-9f97-f58858f9e96e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test RMSE: 0.4534, R²: 0.6914, CI (95%): [0.4285, 0.4771]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ok yeah! This one performs the best and has the highest R^2. Nice."
      ],
      "metadata": {
        "id": "l4PyMLzP5iuX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now I will implement a Bayesian sweep for hyperparam search using WandB"
      ],
      "metadata": {
        "id": "781kJG2Z6LME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "kQkvrILFB0c1",
        "outputId": "7e255c9d-0e5f-4914-a03b-7aed1d19f33b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mraiyann-j\u001b[0m (\u001b[33mraiyann-j-university-of-toronto\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gcn_change4 import GCN\n",
        "\n",
        "sweep_config = {\n",
        "    'method': 'bayes',\n",
        "    'metric': {\n",
        "        'name': 'val_rmse',\n",
        "        'goal': 'minimize'\n",
        "    },\n",
        "    'parameters': {\n",
        "        'epochs': {\n",
        "            'values': [40, 50, 60]\n",
        "        },\n",
        "        'lr': {\n",
        "            'min': 0.0008,\n",
        "            'max': 0.0012,\n",
        "            'distribution': 'uniform'\n",
        "        },\n",
        "        'weight_decay': {\n",
        "            'min': 0.005,\n",
        "            'max': 0.015,\n",
        "            'distribution': 'uniform'\n",
        "        }\n",
        "    }\n",
        "}\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"GCN_Sweep\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "id": "OmZnCfQHCiXE",
        "outputId": "0bcd4c03-f5bd-4289-9632-f4219a7ede2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "updating run config (56s)<br>  <strong style=\"color:red\">ERROR</strong> retrying HTTP 409: run whga4eku was previously created and deleted; try a new run name"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: y5dau394\n",
            "Sweep URL: https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/sweeps/y5dau394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    wandb.init()\n",
        "    config = wandb.config\n",
        "\n",
        "    # Set random seed for reproducibility\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "    num_node = train_graph_list[0].x.shape[1]\n",
        "    edge_attr = train_graph_list[0].edge_attr.shape[1]\n",
        "    u_d = train_graph_list[0].u.shape[1]\n",
        "\n",
        "    model = GCN(num_node_features=num_node,\n",
        "                edge_attr_dim=edge_attr,\n",
        "                u_dim=u_d,\n",
        "                hidden_dim=64,\n",
        "                output_dim=1).to(device)\n",
        "\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-6)\n",
        "\n",
        "    num_epochs = config.epochs\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for data in train_loader:\n",
        "            data = data.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            target = data.y.view(data.num_graphs, -1).to(device)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item() * data.num_graphs\n",
        "        train_loss /= len(train_loader.dataset)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        all_preds, all_targets = [], []\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for data in val_loader:\n",
        "                data = data.to(device)\n",
        "                output = model(data)\n",
        "                target = data.y.view(data.num_graphs, -1).to(device)\n",
        "                loss = criterion(output, target)\n",
        "                val_loss += loss.item() * data.num_graphs\n",
        "                all_preds.extend(output.cpu().numpy())\n",
        "                all_targets.extend(target.cpu().numpy())\n",
        "        val_loss /= len(val_loader.dataset)\n",
        "        val_rmse = val_loss ** 0.5\n",
        "\n",
        "        # Compute R² score\n",
        "        all_preds = np.array(all_preds).flatten()\n",
        "        all_targets = np.array(all_targets).flatten()\n",
        "        r2 = r2_score(all_targets, all_preds)\n",
        "\n",
        "        wandb.log({\n",
        "            \"epoch\": epoch,\n",
        "            \"train_loss\": train_loss,\n",
        "            \"val_rmse\": val_rmse,\n",
        "            \"r2\": r2\n",
        "        }, step=epoch)\n",
        "\n",
        "        scheduler.step()\n",
        "        print(f\"Epoch: {epoch}, Train Loss: {train_loss:.4f}, Val RMSE: {val_rmse:.4f}, R²: {r2:.4f}\")\n",
        "\n",
        "    wandb.finish()"
      ],
      "metadata": {
        "id": "G08WnXM4C0V4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.agent(sweep_id, train, count=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OYEcv4tVDJYL",
        "outputId": "91a875ad-a915-4a20-da97-c6fab98e8c44"
      },
      "execution_count": 17,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: htl877ui with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 60\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.000886187460885644\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.012323978864357\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250327_200523-htl877ui</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/runs/htl877ui' target=\"_blank\">colorful-sweep-1</a></strong> to <a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/sweeps/y5dau394' target=\"_blank\">https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/sweeps/y5dau394</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep' target=\"_blank\">https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/sweeps/y5dau394' target=\"_blank\">https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/sweeps/y5dau394</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/runs/htl877ui' target=\"_blank\">https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/runs/htl877ui</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1, Train Loss: 100.7067, Val RMSE: 1.0571, R²: -0.3042\n",
            "Epoch: 2, Train Loss: 9.7262, Val RMSE: 0.8940, R²: 0.0672\n",
            "Epoch: 3, Train Loss: 3.1292, Val RMSE: 0.8337, R²: 0.1888\n",
            "Epoch: 4, Train Loss: 1.4636, Val RMSE: 0.6743, R²: 0.4694\n",
            "Epoch: 5, Train Loss: 1.0500, Val RMSE: 0.6816, R²: 0.4578\n",
            "Epoch: 6, Train Loss: 0.8153, Val RMSE: 0.6853, R²: 0.4519\n",
            "Epoch: 7, Train Loss: 0.7479, Val RMSE: 0.6579, R²: 0.4948\n",
            "Epoch: 8, Train Loss: 0.6793, Val RMSE: 0.6374, R²: 0.5259\n",
            "Epoch: 9, Train Loss: 0.6625, Val RMSE: 0.6767, R²: 0.4656\n",
            "Epoch: 10, Train Loss: 0.6221, Val RMSE: 0.6362, R²: 0.5276\n",
            "Epoch: 11, Train Loss: 0.6112, Val RMSE: 0.6194, R²: 0.5523\n",
            "Epoch: 12, Train Loss: 0.6048, Val RMSE: 0.6241, R²: 0.5454\n",
            "Epoch: 13, Train Loss: 0.5979, Val RMSE: 0.6498, R²: 0.5073\n",
            "Epoch: 14, Train Loss: 0.6037, Val RMSE: 0.5954, R²: 0.5862\n",
            "Epoch: 15, Train Loss: 0.5841, Val RMSE: 0.5782, R²: 0.6099\n",
            "Epoch: 16, Train Loss: 0.5975, Val RMSE: 0.6333, R²: 0.5319\n",
            "Epoch: 17, Train Loss: 0.5633, Val RMSE: 0.5636, R²: 0.6293\n",
            "Epoch: 18, Train Loss: 0.5557, Val RMSE: 0.5959, R²: 0.5856\n",
            "Epoch: 19, Train Loss: 0.5409, Val RMSE: 0.6050, R²: 0.5728\n",
            "Epoch: 20, Train Loss: 0.5164, Val RMSE: 0.5951, R²: 0.5867\n",
            "Epoch: 21, Train Loss: 0.5297, Val RMSE: 0.5679, R²: 0.6236\n",
            "Epoch: 22, Train Loss: 0.5021, Val RMSE: 0.5641, R²: 0.6287\n",
            "Epoch: 23, Train Loss: 0.5225, Val RMSE: 0.5613, R²: 0.6323\n",
            "Epoch: 24, Train Loss: 0.5103, Val RMSE: 0.5611, R²: 0.6326\n",
            "Epoch: 25, Train Loss: 0.5000, Val RMSE: 0.5341, R²: 0.6670\n",
            "Epoch: 26, Train Loss: 0.4956, Val RMSE: 0.5393, R²: 0.6606\n",
            "Epoch: 27, Train Loss: 0.4921, Val RMSE: 0.5353, R²: 0.6655\n",
            "Epoch: 28, Train Loss: 0.4912, Val RMSE: 0.5341, R²: 0.6671\n",
            "Epoch: 29, Train Loss: 0.4880, Val RMSE: 0.5385, R²: 0.6616\n",
            "Epoch: 30, Train Loss: 0.4920, Val RMSE: 0.4964, R²: 0.7124\n",
            "Epoch: 31, Train Loss: 0.4578, Val RMSE: 0.5488, R²: 0.6485\n",
            "Epoch: 32, Train Loss: 0.4601, Val RMSE: 0.5299, R²: 0.6723\n",
            "Epoch: 33, Train Loss: 0.4629, Val RMSE: 0.5359, R²: 0.6649\n",
            "Epoch: 34, Train Loss: 0.4567, Val RMSE: 0.5293, R²: 0.6731\n",
            "Epoch: 35, Train Loss: 0.4517, Val RMSE: 0.5084, R²: 0.6984\n",
            "Epoch: 36, Train Loss: 0.4422, Val RMSE: 0.5115, R²: 0.6947\n",
            "Epoch: 37, Train Loss: 0.4461, Val RMSE: 0.4997, R²: 0.7086\n",
            "Epoch: 38, Train Loss: 0.4340, Val RMSE: 0.5082, R²: 0.6986\n",
            "Epoch: 39, Train Loss: 0.4347, Val RMSE: 0.5123, R²: 0.6937\n",
            "Epoch: 40, Train Loss: 0.4399, Val RMSE: 0.4949, R²: 0.7142\n",
            "Epoch: 41, Train Loss: 0.4288, Val RMSE: 0.5048, R²: 0.7027\n",
            "Epoch: 42, Train Loss: 0.4391, Val RMSE: 0.5069, R²: 0.7002\n",
            "Epoch: 43, Train Loss: 0.4208, Val RMSE: 0.5047, R²: 0.7027\n",
            "Epoch: 44, Train Loss: 0.4234, Val RMSE: 0.5088, R²: 0.6979\n",
            "Epoch: 45, Train Loss: 0.4238, Val RMSE: 0.5075, R²: 0.6995\n",
            "Epoch: 46, Train Loss: 0.4159, Val RMSE: 0.5108, R²: 0.6955\n",
            "Epoch: 47, Train Loss: 0.4207, Val RMSE: 0.5033, R²: 0.7044\n",
            "Epoch: 48, Train Loss: 0.4247, Val RMSE: 0.5025, R²: 0.7053\n",
            "Epoch: 49, Train Loss: 0.4133, Val RMSE: 0.5053, R²: 0.7020\n",
            "Epoch: 50, Train Loss: 0.4164, Val RMSE: 0.5039, R²: 0.7037\n",
            "Epoch: 51, Train Loss: 0.4065, Val RMSE: 0.5038, R²: 0.7038\n",
            "Epoch: 52, Train Loss: 0.4045, Val RMSE: 0.5036, R²: 0.7040\n",
            "Epoch: 53, Train Loss: 0.4206, Val RMSE: 0.5059, R²: 0.7014\n",
            "Epoch: 54, Train Loss: 0.4186, Val RMSE: 0.5069, R²: 0.7002\n",
            "Epoch: 55, Train Loss: 0.4200, Val RMSE: 0.5064, R²: 0.7007\n",
            "Epoch: 56, Train Loss: 0.4185, Val RMSE: 0.5028, R²: 0.7049\n",
            "Epoch: 57, Train Loss: 0.4137, Val RMSE: 0.5082, R²: 0.6985\n",
            "Epoch: 58, Train Loss: 0.4183, Val RMSE: 0.5020, R²: 0.7059\n",
            "Epoch: 59, Train Loss: 0.4209, Val RMSE: 0.5036, R²: 0.7040\n",
            "Epoch: 60, Train Loss: 0.4059, Val RMSE: 0.5095, R²: 0.6970\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>r2</td><td>▁▄▄▆▆▆▇▆▇▇▇▇▇▇▇█████████████████████████</td></tr><tr><td>train_loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_rmse</td><td>█▆▅▃▃▃▃▃▃▂▃▂▂▂▂▂▂▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>60</td></tr><tr><td>r2</td><td>0.69703</td></tr><tr><td>train_loss</td><td>0.40586</td></tr><tr><td>val_rmse</td><td>0.50952</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">colorful-sweep-1</strong> at: <a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/runs/htl877ui' target=\"_blank\">https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/runs/htl877ui</a><br> View project at: <a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep' target=\"_blank\">https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250327_200523-htl877ui/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: khs9lsrm with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 50\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0011041599374830807\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.011056832182907636\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250327_201250-khs9lsrm</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/runs/khs9lsrm' target=\"_blank\">glorious-sweep-2</a></strong> to <a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/sweeps/y5dau394' target=\"_blank\">https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/sweeps/y5dau394</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep' target=\"_blank\">https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/sweeps/y5dau394' target=\"_blank\">https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/sweeps/y5dau394</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/runs/khs9lsrm' target=\"_blank\">https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/runs/khs9lsrm</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1, Train Loss: 83.4974, Val RMSE: 1.0425, R²: -0.2683\n",
            "Epoch: 2, Train Loss: 6.3893, Val RMSE: 0.9072, R²: 0.0394\n",
            "Epoch: 3, Train Loss: 1.4730, Val RMSE: 0.7037, R²: 0.4221\n",
            "Epoch: 4, Train Loss: 1.0636, Val RMSE: 0.7027, R²: 0.4238\n",
            "Epoch: 5, Train Loss: 0.7913, Val RMSE: 0.6960, R²: 0.4347\n",
            "Epoch: 6, Train Loss: 0.7368, Val RMSE: 0.7141, R²: 0.4048\n",
            "Epoch: 7, Train Loss: 0.6983, Val RMSE: 0.6664, R²: 0.4818\n",
            "Epoch: 8, Train Loss: 0.7014, Val RMSE: 0.6492, R²: 0.5081\n",
            "Epoch: 9, Train Loss: 0.6396, Val RMSE: 0.6850, R²: 0.4524\n",
            "Epoch: 10, Train Loss: 0.6245, Val RMSE: 0.7150, R²: 0.4034\n",
            "Epoch: 11, Train Loss: 0.6481, Val RMSE: 0.6677, R²: 0.4798\n",
            "Epoch: 12, Train Loss: 0.6046, Val RMSE: 0.7502, R²: 0.3432\n",
            "Epoch: 13, Train Loss: 0.6023, Val RMSE: 0.6522, R²: 0.5035\n",
            "Epoch: 14, Train Loss: 0.5706, Val RMSE: 0.6339, R²: 0.5311\n",
            "Epoch: 15, Train Loss: 0.5784, Val RMSE: 0.6032, R²: 0.5754\n",
            "Epoch: 16, Train Loss: 0.5624, Val RMSE: 0.6027, R²: 0.5760\n",
            "Epoch: 17, Train Loss: 0.5422, Val RMSE: 0.6020, R²: 0.5771\n",
            "Epoch: 18, Train Loss: 0.5536, Val RMSE: 0.6633, R²: 0.4866\n",
            "Epoch: 19, Train Loss: 0.5557, Val RMSE: 0.5964, R²: 0.5849\n",
            "Epoch: 20, Train Loss: 0.5271, Val RMSE: 0.6345, R²: 0.5301\n",
            "Epoch: 21, Train Loss: 0.5295, Val RMSE: 0.6318, R²: 0.5342\n",
            "Epoch: 22, Train Loss: 0.5144, Val RMSE: 0.6166, R²: 0.5563\n",
            "Epoch: 23, Train Loss: 0.5239, Val RMSE: 0.5837, R²: 0.6024\n",
            "Epoch: 24, Train Loss: 0.5197, Val RMSE: 0.5974, R²: 0.5835\n",
            "Epoch: 25, Train Loss: 0.5048, Val RMSE: 0.5674, R²: 0.6243\n",
            "Epoch: 26, Train Loss: 0.5191, Val RMSE: 0.5976, R²: 0.5832\n",
            "Epoch: 27, Train Loss: 0.5024, Val RMSE: 0.5625, R²: 0.6308\n",
            "Epoch: 28, Train Loss: 0.4985, Val RMSE: 0.5848, R²: 0.6008\n",
            "Epoch: 29, Train Loss: 0.4886, Val RMSE: 0.5550, R²: 0.6405\n",
            "Epoch: 30, Train Loss: 0.4987, Val RMSE: 0.5533, R²: 0.6427\n",
            "Epoch: 31, Train Loss: 0.4709, Val RMSE: 0.5706, R²: 0.6200\n",
            "Epoch: 32, Train Loss: 0.4791, Val RMSE: 0.5685, R²: 0.6228\n",
            "Epoch: 33, Train Loss: 0.4832, Val RMSE: 0.5536, R²: 0.6424\n",
            "Epoch: 34, Train Loss: 0.4720, Val RMSE: 0.5705, R²: 0.6202\n",
            "Epoch: 35, Train Loss: 0.4557, Val RMSE: 0.5248, R²: 0.6786\n",
            "Epoch: 36, Train Loss: 0.4613, Val RMSE: 0.5385, R²: 0.6616\n",
            "Epoch: 37, Train Loss: 0.4551, Val RMSE: 0.5364, R²: 0.6643\n",
            "Epoch: 38, Train Loss: 0.4431, Val RMSE: 0.5357, R²: 0.6651\n",
            "Epoch: 39, Train Loss: 0.4512, Val RMSE: 0.5318, R²: 0.6699\n",
            "Epoch: 40, Train Loss: 0.4431, Val RMSE: 0.5251, R²: 0.6782\n",
            "Epoch: 41, Train Loss: 0.4427, Val RMSE: 0.5344, R²: 0.6668\n",
            "Epoch: 42, Train Loss: 0.4338, Val RMSE: 0.5332, R²: 0.6682\n",
            "Epoch: 43, Train Loss: 0.4396, Val RMSE: 0.5337, R²: 0.6676\n",
            "Epoch: 44, Train Loss: 0.4325, Val RMSE: 0.5353, R²: 0.6656\n",
            "Epoch: 45, Train Loss: 0.4386, Val RMSE: 0.5308, R²: 0.6712\n",
            "Epoch: 46, Train Loss: 0.4361, Val RMSE: 0.5337, R²: 0.6677\n",
            "Epoch: 47, Train Loss: 0.4230, Val RMSE: 0.5306, R²: 0.6715\n",
            "Epoch: 48, Train Loss: 0.4348, Val RMSE: 0.5302, R²: 0.6719\n",
            "Epoch: 49, Train Loss: 0.4304, Val RMSE: 0.5306, R²: 0.6715\n",
            "Epoch: 50, Train Loss: 0.4241, Val RMSE: 0.5292, R²: 0.6732\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>r2</td><td>▁▃▆▆▆▇▆▆▇▆▇▇▇▇▇▇▇▇▇▇▇█▇█████████████████</td></tr><tr><td>train_loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_rmse</td><td>█▆▃▃▃▃▃▄▃▄▂▂▂▂▃▂▂▂▂▂▂▂▂▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>50</td></tr><tr><td>r2</td><td>0.67318</td></tr><tr><td>train_loss</td><td>0.42406</td></tr><tr><td>val_rmse</td><td>0.5292</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">glorious-sweep-2</strong> at: <a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/runs/khs9lsrm' target=\"_blank\">https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/runs/khs9lsrm</a><br> View project at: <a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep' target=\"_blank\">https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250327_201250-khs9lsrm/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 3f9pd5r6 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 60\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0008868488378563112\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.012100751850523676\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250327_201857-3f9pd5r6</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/runs/3f9pd5r6' target=\"_blank\">still-sweep-3</a></strong> to <a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/sweeps/y5dau394' target=\"_blank\">https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/sweeps/y5dau394</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep' target=\"_blank\">https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/sweeps/y5dau394' target=\"_blank\">https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/sweeps/y5dau394</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/runs/3f9pd5r6' target=\"_blank\">https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/runs/3f9pd5r6</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Train Loss: 100.7375, Val RMSE: 1.0564, R²: -0.3025\n",
            "Epoch: 2, Train Loss: 9.6488, Val RMSE: 0.9110, R²: 0.0315\n",
            "Epoch: 3, Train Loss: 2.9945, Val RMSE: 0.8296, R²: 0.1969\n",
            "Epoch: 4, Train Loss: 1.4274, Val RMSE: 0.6907, R²: 0.4433\n",
            "Epoch: 5, Train Loss: 1.0221, Val RMSE: 0.6827, R²: 0.4561\n",
            "Epoch: 6, Train Loss: 0.8210, Val RMSE: 0.7086, R²: 0.4140\n",
            "Epoch: 7, Train Loss: 0.7968, Val RMSE: 0.6634, R²: 0.4864\n",
            "Epoch: 8, Train Loss: 0.6960, Val RMSE: 0.6463, R²: 0.5126\n",
            "Epoch: 9, Train Loss: 0.6616, Val RMSE: 0.6336, R²: 0.5315\n",
            "Epoch: 10, Train Loss: 0.6324, Val RMSE: 0.6539, R²: 0.5010\n",
            "Epoch: 11, Train Loss: 0.6312, Val RMSE: 0.6265, R²: 0.5419\n",
            "Epoch: 12, Train Loss: 0.5885, Val RMSE: 0.6042, R²: 0.5739\n",
            "Epoch: 13, Train Loss: 0.5757, Val RMSE: 0.6079, R²: 0.5687\n",
            "Epoch: 14, Train Loss: 0.5815, Val RMSE: 0.6194, R²: 0.5522\n",
            "Epoch: 15, Train Loss: 0.5575, Val RMSE: 0.5819, R²: 0.6049\n",
            "Epoch: 16, Train Loss: 0.5425, Val RMSE: 0.5709, R²: 0.6196\n",
            "Epoch: 17, Train Loss: 0.5325, Val RMSE: 0.5716, R²: 0.6187\n",
            "Epoch: 18, Train Loss: 0.5371, Val RMSE: 0.6223, R²: 0.5480\n",
            "Epoch: 19, Train Loss: 0.5258, Val RMSE: 0.5898, R²: 0.5940\n",
            "Epoch: 20, Train Loss: 0.5105, Val RMSE: 0.5570, R²: 0.6379\n",
            "Epoch: 21, Train Loss: 0.5168, Val RMSE: 0.5947, R²: 0.5873\n",
            "Epoch: 22, Train Loss: 0.5004, Val RMSE: 0.5611, R²: 0.6326\n",
            "Epoch: 23, Train Loss: 0.5110, Val RMSE: 0.5401, R²: 0.6596\n",
            "Epoch: 24, Train Loss: 0.4910, Val RMSE: 0.5635, R²: 0.6294\n",
            "Epoch: 25, Train Loss: 0.4786, Val RMSE: 0.5470, R²: 0.6509\n",
            "Epoch: 26, Train Loss: 0.4923, Val RMSE: 0.5718, R²: 0.6184\n",
            "Epoch: 27, Train Loss: 0.4929, Val RMSE: 0.5328, R²: 0.6687\n",
            "Epoch: 28, Train Loss: 0.4862, Val RMSE: 0.5560, R²: 0.6392\n",
            "Epoch: 29, Train Loss: 0.4794, Val RMSE: 0.5249, R²: 0.6785\n",
            "Epoch: 30, Train Loss: 0.4698, Val RMSE: 0.5117, R²: 0.6945\n",
            "Epoch: 31, Train Loss: 0.4487, Val RMSE: 0.5464, R²: 0.6516\n",
            "Epoch: 32, Train Loss: 0.4506, Val RMSE: 0.5439, R²: 0.6547\n",
            "Epoch: 33, Train Loss: 0.4628, Val RMSE: 0.5185, R²: 0.6863\n",
            "Epoch: 34, Train Loss: 0.4550, Val RMSE: 0.5282, R²: 0.6744\n",
            "Epoch: 35, Train Loss: 0.4405, Val RMSE: 0.5084, R²: 0.6984\n",
            "Epoch: 36, Train Loss: 0.4425, Val RMSE: 0.5283, R²: 0.6743\n",
            "Epoch: 37, Train Loss: 0.4339, Val RMSE: 0.5036, R²: 0.7040\n",
            "Epoch: 38, Train Loss: 0.4333, Val RMSE: 0.5086, R²: 0.6982\n",
            "Epoch: 39, Train Loss: 0.4278, Val RMSE: 0.5064, R²: 0.7007\n",
            "Epoch: 40, Train Loss: 0.4353, Val RMSE: 0.4951, R²: 0.7139\n",
            "Epoch: 41, Train Loss: 0.4257, Val RMSE: 0.5091, R²: 0.6975\n",
            "Epoch: 42, Train Loss: 0.4316, Val RMSE: 0.5100, R²: 0.6965\n",
            "Epoch: 43, Train Loss: 0.4180, Val RMSE: 0.5059, R²: 0.7014\n",
            "Epoch: 44, Train Loss: 0.4247, Val RMSE: 0.5056, R²: 0.7017\n",
            "Epoch: 45, Train Loss: 0.4264, Val RMSE: 0.5044, R²: 0.7031\n",
            "Epoch: 46, Train Loss: 0.4248, Val RMSE: 0.5067, R²: 0.7004\n",
            "Epoch: 47, Train Loss: 0.4202, Val RMSE: 0.5037, R²: 0.7039\n",
            "Epoch: 48, Train Loss: 0.4227, Val RMSE: 0.5062, R²: 0.7009\n",
            "Epoch: 49, Train Loss: 0.4218, Val RMSE: 0.5061, R²: 0.7011\n",
            "Epoch: 50, Train Loss: 0.4213, Val RMSE: 0.5047, R²: 0.7028\n",
            "Epoch: 51, Train Loss: 0.4166, Val RMSE: 0.5049, R²: 0.7025\n",
            "Epoch: 52, Train Loss: 0.4115, Val RMSE: 0.5044, R²: 0.7030\n",
            "Epoch: 53, Train Loss: 0.4154, Val RMSE: 0.5045, R²: 0.7029\n",
            "Epoch: 54, Train Loss: 0.4163, Val RMSE: 0.5063, R²: 0.7009\n",
            "Epoch: 55, Train Loss: 0.4189, Val RMSE: 0.5054, R²: 0.7020\n",
            "Epoch: 56, Train Loss: 0.4171, Val RMSE: 0.5035, R²: 0.7041\n",
            "Epoch: 57, Train Loss: 0.4173, Val RMSE: 0.5062, R²: 0.7010\n",
            "Epoch: 58, Train Loss: 0.4238, Val RMSE: 0.5085, R²: 0.6983\n",
            "Epoch: 59, Train Loss: 0.4284, Val RMSE: 0.5043, R²: 0.7032\n",
            "Epoch: 60, Train Loss: 0.4121, Val RMSE: 0.5029, R²: 0.7049\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>r2</td><td>▁▃▅▅▆▆▆▇▆▇▆▇▇▇▇▇██▇▇████████████████████</td></tr><tr><td>train_loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_rmse</td><td>█▆▅▄▃▃▃▃▂▂▂▂▂▃▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>60</td></tr><tr><td>r2</td><td>0.70486</td></tr><tr><td>train_loss</td><td>0.41213</td></tr><tr><td>val_rmse</td><td>0.50289</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">still-sweep-3</strong> at: <a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/runs/3f9pd5r6' target=\"_blank\">https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/runs/3f9pd5r6</a><br> View project at: <a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep' target=\"_blank\">https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250327_201857-3f9pd5r6/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: a5foxdq7 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 40\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001165414220608254\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.014628062036721111\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250327_202628-a5foxdq7</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/runs/a5foxdq7' target=\"_blank\">crisp-sweep-4</a></strong> to <a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/sweeps/y5dau394' target=\"_blank\">https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/sweeps/y5dau394</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep' target=\"_blank\">https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/sweeps/y5dau394' target=\"_blank\">https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/sweeps/y5dau394</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/runs/a5foxdq7' target=\"_blank\">https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/runs/a5foxdq7</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Train Loss: 79.6112, Val RMSE: 1.0137, R²: -0.1992\n",
            "Epoch: 2, Train Loss: 5.7578, Val RMSE: 0.8035, R²: 0.2465\n",
            "Epoch: 3, Train Loss: 1.2716, Val RMSE: 0.6811, R²: 0.4587\n",
            "Epoch: 4, Train Loss: 1.0293, Val RMSE: 0.6743, R²: 0.4694\n",
            "Epoch: 5, Train Loss: 0.7446, Val RMSE: 0.6636, R²: 0.4862\n",
            "Epoch: 6, Train Loss: 0.7092, Val RMSE: 0.6978, R²: 0.4317\n",
            "Epoch: 7, Train Loss: 0.7020, Val RMSE: 0.6848, R²: 0.4527\n",
            "Epoch: 8, Train Loss: 0.6985, Val RMSE: 0.6309, R²: 0.5355\n",
            "Epoch: 9, Train Loss: 0.6344, Val RMSE: 0.7025, R²: 0.4241\n",
            "Epoch: 10, Train Loss: 0.5948, Val RMSE: 0.6499, R²: 0.5071\n",
            "Epoch: 11, Train Loss: 0.5897, Val RMSE: 0.6345, R²: 0.5301\n",
            "Epoch: 12, Train Loss: 0.5724, Val RMSE: 0.6269, R²: 0.5414\n",
            "Epoch: 13, Train Loss: 0.5645, Val RMSE: 0.6086, R²: 0.5677\n",
            "Epoch: 14, Train Loss: 0.5533, Val RMSE: 0.6276, R²: 0.5403\n",
            "Epoch: 15, Train Loss: 0.5664, Val RMSE: 0.5732, R²: 0.6166\n",
            "Epoch: 16, Train Loss: 0.5529, Val RMSE: 0.5687, R²: 0.6226\n",
            "Epoch: 17, Train Loss: 0.5310, Val RMSE: 0.5712, R²: 0.6193\n",
            "Epoch: 18, Train Loss: 0.5429, Val RMSE: 0.6576, R²: 0.4953\n",
            "Epoch: 19, Train Loss: 0.5468, Val RMSE: 0.6201, R²: 0.5513\n",
            "Epoch: 20, Train Loss: 0.5219, Val RMSE: 0.5909, R²: 0.5926\n",
            "Epoch: 21, Train Loss: 0.5245, Val RMSE: 0.6044, R²: 0.5737\n",
            "Epoch: 22, Train Loss: 0.5126, Val RMSE: 0.6043, R²: 0.5738\n",
            "Epoch: 23, Train Loss: 0.5248, Val RMSE: 0.5824, R²: 0.6042\n",
            "Epoch: 24, Train Loss: 0.5228, Val RMSE: 0.6184, R²: 0.5537\n",
            "Epoch: 25, Train Loss: 0.5146, Val RMSE: 0.5666, R²: 0.6254\n",
            "Epoch: 26, Train Loss: 0.5249, Val RMSE: 0.5997, R²: 0.5803\n",
            "Epoch: 27, Train Loss: 0.5114, Val RMSE: 0.5730, R²: 0.6169\n",
            "Epoch: 28, Train Loss: 0.5033, Val RMSE: 0.5749, R²: 0.6143\n",
            "Epoch: 29, Train Loss: 0.4939, Val RMSE: 0.5502, R²: 0.6467\n",
            "Epoch: 30, Train Loss: 0.5068, Val RMSE: 0.5518, R²: 0.6446\n",
            "Epoch: 31, Train Loss: 0.4758, Val RMSE: 0.5920, R²: 0.5911\n",
            "Epoch: 32, Train Loss: 0.4731, Val RMSE: 0.5553, R²: 0.6401\n",
            "Epoch: 33, Train Loss: 0.4994, Val RMSE: 0.5628, R²: 0.6304\n",
            "Epoch: 34, Train Loss: 0.4826, Val RMSE: 0.5635, R²: 0.6295\n",
            "Epoch: 35, Train Loss: 0.4580, Val RMSE: 0.5250, R²: 0.6784\n",
            "Epoch: 36, Train Loss: 0.4615, Val RMSE: 0.5309, R²: 0.6711\n",
            "Epoch: 37, Train Loss: 0.4601, Val RMSE: 0.5250, R²: 0.6783\n",
            "Epoch: 38, Train Loss: 0.4494, Val RMSE: 0.5301, R²: 0.6720\n",
            "Epoch: 39, Train Loss: 0.4509, Val RMSE: 0.5264, R²: 0.6766\n",
            "Epoch: 40, Train Loss: 0.4451, Val RMSE: 0.5199, R²: 0.6845\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>r2</td><td>▁▅▆▆▆▆▆▇▆▇▇▇▇▇▇█▇▇▇▇▇▇▇▇█▇▇▇██▇█████████</td></tr><tr><td>train_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_rmse</td><td>█▅▃▃▃▄▃▃▄▃▃▃▂▃▂▂▂▃▂▂▂▂▂▂▂▂▂▂▁▁▂▂▂▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>40</td></tr><tr><td>r2</td><td>0.68451</td></tr><tr><td>train_loss</td><td>0.44511</td></tr><tr><td>val_rmse</td><td>0.51994</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">crisp-sweep-4</strong> at: <a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/runs/a5foxdq7' target=\"_blank\">https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/runs/a5foxdq7</a><br> View project at: <a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep' target=\"_blank\">https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250327_202628-a5foxdq7/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: h85hyjnj with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 50\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0010106416869554236\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.00675403800099531\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250327_203208-h85hyjnj</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/runs/h85hyjnj' target=\"_blank\">balmy-sweep-5</a></strong> to <a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/sweeps/y5dau394' target=\"_blank\">https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/sweeps/y5dau394</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep' target=\"_blank\">https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/sweeps/y5dau394' target=\"_blank\">https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/sweeps/y5dau394</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/runs/h85hyjnj' target=\"_blank\">https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/runs/h85hyjnj</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Train Loss: 90.0357, Val RMSE: 1.0265, R²: -0.2297\n",
            "Epoch: 2, Train Loss: 7.0563, Val RMSE: 0.8913, R²: 0.0728\n",
            "Epoch: 3, Train Loss: 2.0408, Val RMSE: 0.8367, R²: 0.1829\n",
            "Epoch: 4, Train Loss: 1.2390, Val RMSE: 0.7518, R²: 0.3403\n",
            "Epoch: 5, Train Loss: 1.0122, Val RMSE: 0.7247, R²: 0.3871\n",
            "Epoch: 6, Train Loss: 0.8066, Val RMSE: 0.7432, R²: 0.3553\n",
            "Epoch: 7, Train Loss: 0.7386, Val RMSE: 0.6858, R²: 0.4511\n",
            "Epoch: 8, Train Loss: 0.7419, Val RMSE: 0.7157, R²: 0.4022\n",
            "Epoch: 9, Train Loss: 0.6894, Val RMSE: 0.7025, R²: 0.4241\n",
            "Epoch: 10, Train Loss: 0.6539, Val RMSE: 0.7233, R²: 0.3895\n",
            "Epoch: 11, Train Loss: 0.6319, Val RMSE: 0.6719, R²: 0.4731\n",
            "Epoch: 12, Train Loss: 0.6207, Val RMSE: 0.6590, R²: 0.4931\n",
            "Epoch: 13, Train Loss: 0.6161, Val RMSE: 0.6330, R²: 0.5324\n",
            "Epoch: 14, Train Loss: 0.6103, Val RMSE: 0.6815, R²: 0.4580\n",
            "Epoch: 15, Train Loss: 0.6014, Val RMSE: 0.6370, R²: 0.5264\n",
            "Epoch: 16, Train Loss: 0.5860, Val RMSE: 0.5979, R²: 0.5828\n",
            "Epoch: 17, Train Loss: 0.5836, Val RMSE: 0.6265, R²: 0.5419\n",
            "Epoch: 18, Train Loss: 0.5773, Val RMSE: 0.6623, R²: 0.4881\n",
            "Epoch: 19, Train Loss: 0.5664, Val RMSE: 0.6079, R²: 0.5688\n",
            "Epoch: 20, Train Loss: 0.5487, Val RMSE: 0.6274, R²: 0.5407\n",
            "Epoch: 21, Train Loss: 0.5541, Val RMSE: 0.5780, R²: 0.6101\n",
            "Epoch: 22, Train Loss: 0.5348, Val RMSE: 0.5920, R²: 0.5910\n",
            "Epoch: 23, Train Loss: 0.5413, Val RMSE: 0.5959, R²: 0.5856\n",
            "Epoch: 24, Train Loss: 0.5584, Val RMSE: 0.5961, R²: 0.5853\n",
            "Epoch: 25, Train Loss: 0.5202, Val RMSE: 0.5594, R²: 0.6348\n",
            "Epoch: 26, Train Loss: 0.5289, Val RMSE: 0.5849, R²: 0.6007\n",
            "Epoch: 27, Train Loss: 0.5348, Val RMSE: 0.5586, R²: 0.6358\n",
            "Epoch: 28, Train Loss: 0.5153, Val RMSE: 0.6030, R²: 0.5756\n",
            "Epoch: 29, Train Loss: 0.5172, Val RMSE: 0.5553, R²: 0.6402\n",
            "Epoch: 30, Train Loss: 0.5154, Val RMSE: 0.5268, R²: 0.6761\n",
            "Epoch: 31, Train Loss: 0.4868, Val RMSE: 0.5683, R²: 0.6231\n",
            "Epoch: 32, Train Loss: 0.4892, Val RMSE: 0.5681, R²: 0.6234\n",
            "Epoch: 33, Train Loss: 0.4824, Val RMSE: 0.5392, R²: 0.6607\n",
            "Epoch: 34, Train Loss: 0.4583, Val RMSE: 0.5399, R²: 0.6599\n",
            "Epoch: 35, Train Loss: 0.4548, Val RMSE: 0.5231, R²: 0.6806\n",
            "Epoch: 36, Train Loss: 0.4642, Val RMSE: 0.5349, R²: 0.6661\n",
            "Epoch: 37, Train Loss: 0.4403, Val RMSE: 0.5164, R²: 0.6888\n",
            "Epoch: 38, Train Loss: 0.4419, Val RMSE: 0.5255, R²: 0.6777\n",
            "Epoch: 39, Train Loss: 0.4462, Val RMSE: 0.5256, R²: 0.6776\n",
            "Epoch: 40, Train Loss: 0.4414, Val RMSE: 0.5198, R²: 0.6847\n",
            "Epoch: 41, Train Loss: 0.4373, Val RMSE: 0.5338, R²: 0.6675\n",
            "Epoch: 42, Train Loss: 0.4367, Val RMSE: 0.5438, R²: 0.6549\n",
            "Epoch: 43, Train Loss: 0.4336, Val RMSE: 0.5326, R²: 0.6689\n",
            "Epoch: 44, Train Loss: 0.4275, Val RMSE: 0.5283, R²: 0.6743\n",
            "Epoch: 45, Train Loss: 0.4306, Val RMSE: 0.5350, R²: 0.6660\n",
            "Epoch: 46, Train Loss: 0.4294, Val RMSE: 0.5338, R²: 0.6675\n",
            "Epoch: 47, Train Loss: 0.4309, Val RMSE: 0.5344, R²: 0.6667\n",
            "Epoch: 48, Train Loss: 0.4350, Val RMSE: 0.5339, R²: 0.6674\n",
            "Epoch: 49, Train Loss: 0.4281, Val RMSE: 0.5352, R²: 0.6657\n",
            "Epoch: 50, Train Loss: 0.4253, Val RMSE: 0.5338, R²: 0.6674\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>r2</td><td>▁▃▄▅▅▆▆▆▆▇▆▇▇▇▆▇▇▇▇▇█▇██▇███████████████</td></tr><tr><td>train_loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_rmse</td><td>█▅▄▄▄▄▄▄▃▃▃▃▂▃▃▃▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>50</td></tr><tr><td>r2</td><td>0.66741</td></tr><tr><td>train_loss</td><td>0.42529</td></tr><tr><td>val_rmse</td><td>0.53385</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">balmy-sweep-5</strong> at: <a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/runs/h85hyjnj' target=\"_blank\">https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/runs/h85hyjnj</a><br> View project at: <a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep' target=\"_blank\">https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250327_203208-h85hyjnj/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 191byxkq with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 60\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0009105444714952368\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.012601351515500445\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250327_203838-191byxkq</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/runs/191byxkq' target=\"_blank\">fresh-sweep-6</a></strong> to <a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/sweeps/y5dau394' target=\"_blank\">https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/sweeps/y5dau394</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep' target=\"_blank\">https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/sweeps/y5dau394' target=\"_blank\">https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/sweeps/y5dau394</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/runs/191byxkq' target=\"_blank\">https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/runs/191byxkq</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Train Loss: 98.4241, Val RMSE: 1.0475, R²: -0.2805\n",
            "Epoch: 2, Train Loss: 9.1362, Val RMSE: 0.9007, R²: 0.0533\n",
            "Epoch: 3, Train Loss: 2.5265, Val RMSE: 0.6996, R²: 0.4288\n",
            "Epoch: 4, Train Loss: 1.3577, Val RMSE: 0.7105, R²: 0.4109\n",
            "Epoch: 5, Train Loss: 0.9751, Val RMSE: 0.6745, R²: 0.4691\n",
            "Epoch: 6, Train Loss: 0.8198, Val RMSE: 0.7158, R²: 0.4021\n",
            "Epoch: 7, Train Loss: 0.6983, Val RMSE: 0.6532, R²: 0.5021\n",
            "Epoch: 8, Train Loss: 0.6728, Val RMSE: 0.6684, R²: 0.4786\n",
            "Epoch: 9, Train Loss: 0.6488, Val RMSE: 0.6676, R²: 0.4799\n",
            "Epoch: 10, Train Loss: 0.6056, Val RMSE: 0.6325, R²: 0.5331\n",
            "Epoch: 11, Train Loss: 0.5995, Val RMSE: 0.6261, R²: 0.5425\n",
            "Epoch: 12, Train Loss: 0.5774, Val RMSE: 0.6299, R²: 0.5370\n",
            "Epoch: 13, Train Loss: 0.5794, Val RMSE: 0.6180, R²: 0.5543\n",
            "Epoch: 14, Train Loss: 0.5749, Val RMSE: 0.5929, R²: 0.5898\n",
            "Epoch: 15, Train Loss: 0.5565, Val RMSE: 0.5818, R²: 0.6050\n",
            "Epoch: 16, Train Loss: 0.5496, Val RMSE: 0.5747, R²: 0.6145\n",
            "Epoch: 17, Train Loss: 0.5268, Val RMSE: 0.5671, R²: 0.6247\n",
            "Epoch: 18, Train Loss: 0.5289, Val RMSE: 0.6284, R²: 0.5391\n",
            "Epoch: 19, Train Loss: 0.5210, Val RMSE: 0.5909, R²: 0.5925\n",
            "Epoch: 20, Train Loss: 0.4954, Val RMSE: 0.5594, R²: 0.6348\n",
            "Epoch: 21, Train Loss: 0.5011, Val RMSE: 0.5686, R²: 0.6227\n",
            "Epoch: 22, Train Loss: 0.4860, Val RMSE: 0.5471, R²: 0.6507\n",
            "Epoch: 23, Train Loss: 0.5052, Val RMSE: 0.5585, R²: 0.6360\n",
            "Epoch: 24, Train Loss: 0.4807, Val RMSE: 0.5468, R²: 0.6511\n",
            "Epoch: 25, Train Loss: 0.4782, Val RMSE: 0.5409, R²: 0.6586\n",
            "Epoch: 26, Train Loss: 0.4789, Val RMSE: 0.5707, R²: 0.6199\n",
            "Epoch: 27, Train Loss: 0.4697, Val RMSE: 0.5240, R²: 0.6796\n",
            "Epoch: 28, Train Loss: 0.4756, Val RMSE: 0.5411, R²: 0.6583\n",
            "Epoch: 29, Train Loss: 0.4678, Val RMSE: 0.5233, R²: 0.6804\n",
            "Epoch: 30, Train Loss: 0.4708, Val RMSE: 0.5108, R²: 0.6955\n",
            "Epoch: 31, Train Loss: 0.4341, Val RMSE: 0.5491, R²: 0.6481\n",
            "Epoch: 32, Train Loss: 0.4411, Val RMSE: 0.5488, R²: 0.6485\n",
            "Epoch: 33, Train Loss: 0.4491, Val RMSE: 0.5332, R²: 0.6682\n",
            "Epoch: 34, Train Loss: 0.4454, Val RMSE: 0.5323, R²: 0.6693\n",
            "Epoch: 35, Train Loss: 0.4265, Val RMSE: 0.5082, R²: 0.6986\n",
            "Epoch: 36, Train Loss: 0.4253, Val RMSE: 0.5109, R²: 0.6954\n",
            "Epoch: 37, Train Loss: 0.4299, Val RMSE: 0.5070, R²: 0.7000\n",
            "Epoch: 38, Train Loss: 0.4198, Val RMSE: 0.5148, R²: 0.6907\n",
            "Epoch: 39, Train Loss: 0.4180, Val RMSE: 0.5140, R²: 0.6917\n",
            "Epoch: 40, Train Loss: 0.4219, Val RMSE: 0.5007, R²: 0.7074\n",
            "Epoch: 41, Train Loss: 0.4115, Val RMSE: 0.5111, R²: 0.6952\n",
            "Epoch: 42, Train Loss: 0.4216, Val RMSE: 0.5116, R²: 0.6946\n",
            "Epoch: 43, Train Loss: 0.4078, Val RMSE: 0.5075, R²: 0.6994\n",
            "Epoch: 44, Train Loss: 0.4129, Val RMSE: 0.5078, R²: 0.6990\n",
            "Epoch: 45, Train Loss: 0.4162, Val RMSE: 0.5059, R²: 0.7013\n",
            "Epoch: 46, Train Loss: 0.4056, Val RMSE: 0.5110, R²: 0.6952\n",
            "Epoch: 47, Train Loss: 0.4077, Val RMSE: 0.5073, R²: 0.6997\n",
            "Epoch: 48, Train Loss: 0.4086, Val RMSE: 0.5093, R²: 0.6973\n",
            "Epoch: 49, Train Loss: 0.4020, Val RMSE: 0.5086, R²: 0.6982\n",
            "Epoch: 50, Train Loss: 0.4068, Val RMSE: 0.5063, R²: 0.7008\n",
            "Epoch: 51, Train Loss: 0.3954, Val RMSE: 0.5066, R²: 0.7005\n",
            "Epoch: 52, Train Loss: 0.3990, Val RMSE: 0.5064, R²: 0.7007\n",
            "Epoch: 53, Train Loss: 0.4032, Val RMSE: 0.5075, R²: 0.6995\n",
            "Epoch: 54, Train Loss: 0.4019, Val RMSE: 0.5073, R²: 0.6997\n",
            "Epoch: 55, Train Loss: 0.4113, Val RMSE: 0.5074, R²: 0.6995\n",
            "Epoch: 56, Train Loss: 0.4055, Val RMSE: 0.5070, R²: 0.7000\n",
            "Epoch: 57, Train Loss: 0.4043, Val RMSE: 0.5085, R²: 0.6982\n",
            "Epoch: 58, Train Loss: 0.4110, Val RMSE: 0.5094, R²: 0.6972\n",
            "Epoch: 59, Train Loss: 0.4149, Val RMSE: 0.5048, R²: 0.7026\n",
            "Epoch: 60, Train Loss: 0.3946, Val RMSE: 0.5036, R²: 0.7040\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>r2</td><td>▁▁▂▃▃▄▄▄▅▆▄▅▆▆▇▇▇▆▇▇█▇▇▇████████████████</td></tr><tr><td>train_loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_rmse</td><td>█▄▄▄▃▃▃▃▂▂▂▂▃▂▂▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>60</td></tr><tr><td>r2</td><td>0.70405</td></tr><tr><td>train_loss</td><td>0.39464</td></tr><tr><td>val_rmse</td><td>0.50358</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">fresh-sweep-6</strong> at: <a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/runs/191byxkq' target=\"_blank\">https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/runs/191byxkq</a><br> View project at: <a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep' target=\"_blank\">https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250327_203838-191byxkq/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 2y3yytt5 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 60\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0008935532131641973\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.006955968863499973\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250327_204559-2y3yytt5</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/runs/2y3yytt5' target=\"_blank\">leafy-sweep-7</a></strong> to <a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/sweeps/y5dau394' target=\"_blank\">https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/sweeps/y5dau394</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep' target=\"_blank\">https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/sweeps/y5dau394' target=\"_blank\">https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/sweeps/y5dau394</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/runs/2y3yytt5' target=\"_blank\">https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/runs/2y3yytt5</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Train Loss: 100.1350, Val RMSE: 1.0431, R²: -0.2697\n",
            "Epoch: 2, Train Loss: 9.1208, Val RMSE: 0.8993, R²: 0.0563\n",
            "Epoch: 3, Train Loss: 2.8384, Val RMSE: 0.6910, R²: 0.4428\n",
            "Epoch: 4, Train Loss: 1.4413, Val RMSE: 0.7158, R²: 0.4021\n",
            "Epoch: 5, Train Loss: 1.0858, Val RMSE: 0.6777, R²: 0.4640\n",
            "Epoch: 6, Train Loss: 0.8368, Val RMSE: 0.7268, R²: 0.3835\n",
            "Epoch: 7, Train Loss: 0.7433, Val RMSE: 0.6463, R²: 0.5126\n",
            "Epoch: 8, Train Loss: 0.6762, Val RMSE: 0.6665, R²: 0.4816\n",
            "Epoch: 9, Train Loss: 0.6606, Val RMSE: 0.6773, R²: 0.4647\n",
            "Epoch: 10, Train Loss: 0.6161, Val RMSE: 0.6828, R²: 0.4559\n",
            "Epoch: 11, Train Loss: 0.6082, Val RMSE: 0.6355, R²: 0.5287\n",
            "Epoch: 12, Train Loss: 0.5769, Val RMSE: 0.6395, R²: 0.5228\n",
            "Epoch: 13, Train Loss: 0.5691, Val RMSE: 0.6337, R²: 0.5314\n",
            "Epoch: 14, Train Loss: 0.5761, Val RMSE: 0.6206, R²: 0.5506\n",
            "Epoch: 15, Train Loss: 0.5578, Val RMSE: 0.5972, R²: 0.5838\n",
            "Epoch: 16, Train Loss: 0.5612, Val RMSE: 0.6063, R²: 0.5710\n",
            "Epoch: 17, Train Loss: 0.5241, Val RMSE: 0.5621, R²: 0.6312\n",
            "Epoch: 18, Train Loss: 0.5205, Val RMSE: 0.6035, R²: 0.5749\n",
            "Epoch: 19, Train Loss: 0.5031, Val RMSE: 0.5909, R²: 0.5926\n",
            "Epoch: 20, Train Loss: 0.4930, Val RMSE: 0.5713, R²: 0.6191\n",
            "Epoch: 21, Train Loss: 0.4993, Val RMSE: 0.5544, R²: 0.6413\n",
            "Epoch: 22, Train Loss: 0.4797, Val RMSE: 0.5457, R²: 0.6525\n",
            "Epoch: 23, Train Loss: 0.5005, Val RMSE: 0.5454, R²: 0.6529\n",
            "Epoch: 24, Train Loss: 0.4869, Val RMSE: 0.5793, R²: 0.6083\n",
            "Epoch: 25, Train Loss: 0.4731, Val RMSE: 0.5499, R²: 0.6471\n",
            "Epoch: 26, Train Loss: 0.4741, Val RMSE: 0.5384, R²: 0.6617\n",
            "Epoch: 27, Train Loss: 0.4769, Val RMSE: 0.5573, R²: 0.6375\n",
            "Epoch: 28, Train Loss: 0.4717, Val RMSE: 0.5318, R²: 0.6700\n",
            "Epoch: 29, Train Loss: 0.4647, Val RMSE: 0.5348, R²: 0.6663\n",
            "Epoch: 30, Train Loss: 0.4689, Val RMSE: 0.5067, R²: 0.7004\n",
            "Epoch: 31, Train Loss: 0.4353, Val RMSE: 0.5556, R²: 0.6398\n",
            "Epoch: 32, Train Loss: 0.4389, Val RMSE: 0.5423, R²: 0.6569\n",
            "Epoch: 33, Train Loss: 0.4400, Val RMSE: 0.5267, R²: 0.6763\n",
            "Epoch: 34, Train Loss: 0.4388, Val RMSE: 0.5285, R²: 0.6741\n",
            "Epoch: 35, Train Loss: 0.4239, Val RMSE: 0.5047, R²: 0.7028\n",
            "Epoch: 36, Train Loss: 0.4262, Val RMSE: 0.5067, R²: 0.7004\n",
            "Epoch: 37, Train Loss: 0.4143, Val RMSE: 0.4905, R²: 0.7192\n",
            "Epoch: 38, Train Loss: 0.4218, Val RMSE: 0.5028, R²: 0.7049\n",
            "Epoch: 39, Train Loss: 0.4149, Val RMSE: 0.4971, R²: 0.7116\n",
            "Epoch: 40, Train Loss: 0.4145, Val RMSE: 0.4894, R²: 0.7205\n",
            "Epoch: 41, Train Loss: 0.4062, Val RMSE: 0.5024, R²: 0.7054\n",
            "Epoch: 42, Train Loss: 0.4165, Val RMSE: 0.5058, R²: 0.7014\n",
            "Epoch: 43, Train Loss: 0.4042, Val RMSE: 0.4963, R²: 0.7126\n",
            "Epoch: 44, Train Loss: 0.4092, Val RMSE: 0.5003, R²: 0.7079\n",
            "Epoch: 45, Train Loss: 0.4069, Val RMSE: 0.5002, R²: 0.7080\n",
            "Epoch: 46, Train Loss: 0.4018, Val RMSE: 0.5027, R²: 0.7050\n",
            "Epoch: 47, Train Loss: 0.4040, Val RMSE: 0.5002, R²: 0.7080\n",
            "Epoch: 48, Train Loss: 0.4072, Val RMSE: 0.4987, R²: 0.7098\n",
            "Epoch: 49, Train Loss: 0.3965, Val RMSE: 0.5004, R²: 0.7078\n",
            "Epoch: 50, Train Loss: 0.3983, Val RMSE: 0.4983, R²: 0.7103\n",
            "Epoch: 51, Train Loss: 0.3939, Val RMSE: 0.4986, R²: 0.7099\n",
            "Epoch: 52, Train Loss: 0.3935, Val RMSE: 0.4983, R²: 0.7102\n",
            "Epoch: 53, Train Loss: 0.3957, Val RMSE: 0.4987, R²: 0.7097\n",
            "Epoch: 54, Train Loss: 0.3992, Val RMSE: 0.5016, R²: 0.7063\n",
            "Epoch: 55, Train Loss: 0.4037, Val RMSE: 0.5019, R²: 0.7061\n",
            "Epoch: 56, Train Loss: 0.3979, Val RMSE: 0.4989, R²: 0.7096\n",
            "Epoch: 57, Train Loss: 0.3984, Val RMSE: 0.5018, R²: 0.7061\n",
            "Epoch: 58, Train Loss: 0.4038, Val RMSE: 0.4975, R²: 0.7112\n",
            "Epoch: 59, Train Loss: 0.4060, Val RMSE: 0.4984, R²: 0.7101\n",
            "Epoch: 60, Train Loss: 0.3908, Val RMSE: 0.5002, R²: 0.7080\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>r2</td><td>▁▆▆▇▆▆▇▇▇▇▇▇▇▇█▇▇▇██████████████████████</td></tr><tr><td>train_loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_rmse</td><td>█▄▅▄▅▄▄▃▄▃▃▂▃▃▂▂▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>60</td></tr><tr><td>r2</td><td>0.70799</td></tr><tr><td>train_loss</td><td>0.39077</td></tr><tr><td>val_rmse</td><td>0.50022</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">leafy-sweep-7</strong> at: <a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/runs/2y3yytt5' target=\"_blank\">https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/runs/2y3yytt5</a><br> View project at: <a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep' target=\"_blank\">https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250327_204559-2y3yytt5/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: nlnclbeb with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 50\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001057138036105804\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.01478506977883988\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250327_205326-nlnclbeb</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/runs/nlnclbeb' target=\"_blank\">floral-sweep-8</a></strong> to <a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/sweeps/y5dau394' target=\"_blank\">https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/sweeps/y5dau394</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep' target=\"_blank\">https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/sweeps/y5dau394' target=\"_blank\">https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/sweeps/y5dau394</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/runs/nlnclbeb' target=\"_blank\">https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/runs/nlnclbeb</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Train Loss: 86.5968, Val RMSE: 1.0302, R²: -0.2385\n",
            "Epoch: 2, Train Loss: 6.8707, Val RMSE: 0.9013, R²: 0.0520\n",
            "Epoch: 3, Train Loss: 1.6507, Val RMSE: 0.7339, R²: 0.3714\n",
            "Epoch: 4, Train Loss: 1.0812, Val RMSE: 0.7082, R²: 0.4147\n",
            "Epoch: 5, Train Loss: 0.8512, Val RMSE: 0.6842, R²: 0.4536\n",
            "Epoch: 6, Train Loss: 0.7687, Val RMSE: 0.7121, R²: 0.4082\n",
            "Epoch: 7, Train Loss: 0.7012, Val RMSE: 0.6571, R²: 0.4961\n",
            "Epoch: 8, Train Loss: 0.6706, Val RMSE: 0.6738, R²: 0.4702\n",
            "Epoch: 9, Train Loss: 0.6384, Val RMSE: 0.6640, R²: 0.4855\n",
            "Epoch: 10, Train Loss: 0.6093, Val RMSE: 0.6566, R²: 0.4969\n",
            "Epoch: 11, Train Loss: 0.5944, Val RMSE: 0.6172, R²: 0.5555\n",
            "Epoch: 12, Train Loss: 0.5781, Val RMSE: 0.6290, R²: 0.5383\n",
            "Epoch: 13, Train Loss: 0.5712, Val RMSE: 0.6266, R²: 0.5418\n",
            "Epoch: 14, Train Loss: 0.5575, Val RMSE: 0.5817, R²: 0.6051\n",
            "Epoch: 15, Train Loss: 0.5681, Val RMSE: 0.5878, R²: 0.5968\n",
            "Epoch: 16, Train Loss: 0.5588, Val RMSE: 0.6113, R²: 0.5639\n",
            "Epoch: 17, Train Loss: 0.5418, Val RMSE: 0.5559, R²: 0.6393\n",
            "Epoch: 18, Train Loss: 0.5319, Val RMSE: 0.6294, R²: 0.5377\n",
            "Epoch: 19, Train Loss: 0.5248, Val RMSE: 0.5896, R²: 0.5943\n",
            "Epoch: 20, Train Loss: 0.5101, Val RMSE: 0.5944, R²: 0.5877\n",
            "Epoch: 21, Train Loss: 0.5188, Val RMSE: 0.5877, R²: 0.5969\n",
            "Epoch: 22, Train Loss: 0.5037, Val RMSE: 0.5677, R²: 0.6238\n",
            "Epoch: 23, Train Loss: 0.5044, Val RMSE: 0.5314, R²: 0.6704\n",
            "Epoch: 24, Train Loss: 0.5085, Val RMSE: 0.5464, R²: 0.6516\n",
            "Epoch: 25, Train Loss: 0.4848, Val RMSE: 0.5284, R²: 0.6741\n",
            "Epoch: 26, Train Loss: 0.4927, Val RMSE: 0.5861, R²: 0.5992\n",
            "Epoch: 27, Train Loss: 0.4888, Val RMSE: 0.5201, R²: 0.6843\n",
            "Epoch: 28, Train Loss: 0.4775, Val RMSE: 0.5410, R²: 0.6585\n",
            "Epoch: 29, Train Loss: 0.4730, Val RMSE: 0.5214, R²: 0.6827\n",
            "Epoch: 30, Train Loss: 0.4719, Val RMSE: 0.5100, R²: 0.6964\n",
            "Epoch: 31, Train Loss: 0.4502, Val RMSE: 0.5574, R²: 0.6374\n",
            "Epoch: 32, Train Loss: 0.4559, Val RMSE: 0.5359, R²: 0.6649\n",
            "Epoch: 33, Train Loss: 0.4606, Val RMSE: 0.5364, R²: 0.6642\n",
            "Epoch: 34, Train Loss: 0.4463, Val RMSE: 0.5256, R²: 0.6776\n",
            "Epoch: 35, Train Loss: 0.4298, Val RMSE: 0.5048, R²: 0.7026\n",
            "Epoch: 36, Train Loss: 0.4402, Val RMSE: 0.5082, R²: 0.6986\n",
            "Epoch: 37, Train Loss: 0.4264, Val RMSE: 0.4915, R²: 0.7181\n",
            "Epoch: 38, Train Loss: 0.4214, Val RMSE: 0.5089, R²: 0.6978\n",
            "Epoch: 39, Train Loss: 0.4241, Val RMSE: 0.5012, R²: 0.7068\n",
            "Epoch: 40, Train Loss: 0.4192, Val RMSE: 0.4922, R²: 0.7173\n",
            "Epoch: 41, Train Loss: 0.4157, Val RMSE: 0.5145, R²: 0.6911\n",
            "Epoch: 42, Train Loss: 0.4128, Val RMSE: 0.5057, R²: 0.7015\n",
            "Epoch: 43, Train Loss: 0.4135, Val RMSE: 0.5043, R²: 0.7031\n",
            "Epoch: 44, Train Loss: 0.4123, Val RMSE: 0.5055, R²: 0.7018\n",
            "Epoch: 45, Train Loss: 0.4113, Val RMSE: 0.5045, R²: 0.7030\n",
            "Epoch: 46, Train Loss: 0.4077, Val RMSE: 0.5057, R²: 0.7015\n",
            "Epoch: 47, Train Loss: 0.4054, Val RMSE: 0.5046, R²: 0.7028\n",
            "Epoch: 48, Train Loss: 0.4134, Val RMSE: 0.5048, R²: 0.7026\n",
            "Epoch: 49, Train Loss: 0.4082, Val RMSE: 0.5050, R²: 0.7024\n",
            "Epoch: 50, Train Loss: 0.4028, Val RMSE: 0.5039, R²: 0.7037\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>r2</td><td>▁▃▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇██▇███████████████████</td></tr><tr><td>train_loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_rmse</td><td>█▄▄▄▄▃▃▃▃▃▂▃▂▃▂▂▂▂▂▁▁▂▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>50</td></tr><tr><td>r2</td><td>0.70365</td></tr><tr><td>train_loss</td><td>0.40281</td></tr><tr><td>val_rmse</td><td>0.50392</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">floral-sweep-8</strong> at: <a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/runs/nlnclbeb' target=\"_blank\">https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/runs/nlnclbeb</a><br> View project at: <a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep' target=\"_blank\">https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250327_205326-nlnclbeb/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: mkkc11g4 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 50\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0010890883200160078\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.00797417825416806\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250327_205942-mkkc11g4</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/runs/mkkc11g4' target=\"_blank\">clear-sweep-9</a></strong> to <a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/sweeps/y5dau394' target=\"_blank\">https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/sweeps/y5dau394</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep' target=\"_blank\">https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/sweeps/y5dau394' target=\"_blank\">https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/sweeps/y5dau394</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/runs/mkkc11g4' target=\"_blank\">https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/runs/mkkc11g4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Train Loss: 84.4610, Val RMSE: 1.0149, R²: -0.2020\n",
            "Epoch: 2, Train Loss: 6.2977, Val RMSE: 0.9085, R²: 0.0369\n",
            "Epoch: 3, Train Loss: 1.5649, Val RMSE: 0.7255, R²: 0.3857\n",
            "Epoch: 4, Train Loss: 1.0519, Val RMSE: 0.6868, R²: 0.4496\n",
            "Epoch: 5, Train Loss: 0.7885, Val RMSE: 0.6789, R²: 0.4621\n",
            "Epoch: 6, Train Loss: 0.6966, Val RMSE: 0.6869, R²: 0.4494\n",
            "Epoch: 7, Train Loss: 0.6788, Val RMSE: 0.6459, R²: 0.5132\n",
            "Epoch: 8, Train Loss: 0.6608, Val RMSE: 0.6756, R²: 0.4673\n",
            "Epoch: 9, Train Loss: 0.6251, Val RMSE: 0.6966, R²: 0.4337\n",
            "Epoch: 10, Train Loss: 0.6162, Val RMSE: 0.7057, R²: 0.4189\n",
            "Epoch: 11, Train Loss: 0.6333, Val RMSE: 0.6497, R²: 0.5074\n",
            "Epoch: 12, Train Loss: 0.5888, Val RMSE: 0.6481, R²: 0.5098\n",
            "Epoch: 13, Train Loss: 0.5796, Val RMSE: 0.6333, R²: 0.5320\n",
            "Epoch: 14, Train Loss: 0.5605, Val RMSE: 0.6187, R²: 0.5533\n",
            "Epoch: 15, Train Loss: 0.5683, Val RMSE: 0.5937, R²: 0.5886\n",
            "Epoch: 16, Train Loss: 0.5687, Val RMSE: 0.6035, R²: 0.5750\n",
            "Epoch: 17, Train Loss: 0.5398, Val RMSE: 0.5679, R²: 0.6236\n",
            "Epoch: 18, Train Loss: 0.5312, Val RMSE: 0.6284, R²: 0.5391\n",
            "Epoch: 19, Train Loss: 0.5268, Val RMSE: 0.6086, R²: 0.5677\n",
            "Epoch: 20, Train Loss: 0.5136, Val RMSE: 0.6019, R²: 0.5772\n",
            "Epoch: 21, Train Loss: 0.5233, Val RMSE: 0.6049, R²: 0.5731\n",
            "Epoch: 22, Train Loss: 0.5044, Val RMSE: 0.5888, R²: 0.5954\n",
            "Epoch: 23, Train Loss: 0.5154, Val RMSE: 0.5525, R²: 0.6438\n",
            "Epoch: 24, Train Loss: 0.5083, Val RMSE: 0.5908, R²: 0.5927\n",
            "Epoch: 25, Train Loss: 0.4971, Val RMSE: 0.5513, R²: 0.6454\n",
            "Epoch: 26, Train Loss: 0.5065, Val RMSE: 0.5763, R²: 0.6125\n",
            "Epoch: 27, Train Loss: 0.4985, Val RMSE: 0.5391, R²: 0.6609\n",
            "Epoch: 28, Train Loss: 0.4898, Val RMSE: 0.5850, R²: 0.6007\n",
            "Epoch: 29, Train Loss: 0.4873, Val RMSE: 0.5401, R²: 0.6595\n",
            "Epoch: 30, Train Loss: 0.4928, Val RMSE: 0.5299, R²: 0.6723\n",
            "Epoch: 31, Train Loss: 0.4623, Val RMSE: 0.5776, R²: 0.6106\n",
            "Epoch: 32, Train Loss: 0.4700, Val RMSE: 0.5507, R²: 0.6461\n",
            "Epoch: 33, Train Loss: 0.4757, Val RMSE: 0.5390, R²: 0.6610\n",
            "Epoch: 34, Train Loss: 0.4657, Val RMSE: 0.5598, R²: 0.6343\n",
            "Epoch: 35, Train Loss: 0.4573, Val RMSE: 0.5224, R²: 0.6815\n",
            "Epoch: 36, Train Loss: 0.4588, Val RMSE: 0.5378, R²: 0.6625\n",
            "Epoch: 37, Train Loss: 0.4513, Val RMSE: 0.5244, R²: 0.6791\n",
            "Epoch: 38, Train Loss: 0.4402, Val RMSE: 0.5266, R²: 0.6764\n",
            "Epoch: 39, Train Loss: 0.4480, Val RMSE: 0.5177, R²: 0.6872\n",
            "Epoch: 40, Train Loss: 0.4412, Val RMSE: 0.5128, R²: 0.6931\n",
            "Epoch: 41, Train Loss: 0.4414, Val RMSE: 0.5234, R²: 0.6803\n",
            "Epoch: 42, Train Loss: 0.4325, Val RMSE: 0.5221, R²: 0.6819\n",
            "Epoch: 43, Train Loss: 0.4369, Val RMSE: 0.5195, R²: 0.6850\n",
            "Epoch: 44, Train Loss: 0.4314, Val RMSE: 0.5180, R²: 0.6868\n",
            "Epoch: 45, Train Loss: 0.4348, Val RMSE: 0.5140, R²: 0.6917\n",
            "Epoch: 46, Train Loss: 0.4342, Val RMSE: 0.5191, R²: 0.6855\n",
            "Epoch: 47, Train Loss: 0.4246, Val RMSE: 0.5195, R²: 0.6850\n",
            "Epoch: 48, Train Loss: 0.4329, Val RMSE: 0.5182, R²: 0.6867\n",
            "Epoch: 49, Train Loss: 0.4265, Val RMSE: 0.5187, R²: 0.6860\n",
            "Epoch: 50, Train Loss: 0.4241, Val RMSE: 0.5175, R²: 0.6875\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>r2</td><td>▁▃▆▆▆▇▆▆▇▇▇▇▇▇▇▇▇█▇██▇██▇███████████████</td></tr><tr><td>train_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_rmse</td><td>█▇▄▃▃▃▃▄▃▃▂▂▂▂▃▂▂▂▂▂▂▁▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>50</td></tr><tr><td>r2</td><td>0.68749</td></tr><tr><td>train_loss</td><td>0.42413</td></tr><tr><td>val_rmse</td><td>0.51748</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">clear-sweep-9</strong> at: <a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/runs/mkkc11g4' target=\"_blank\">https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/runs/mkkc11g4</a><br> View project at: <a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep' target=\"_blank\">https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250327_205942-mkkc11g4/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: zdfxdg5v with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 50\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.00080466220371861\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.01085875929346324\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250327_210623-zdfxdg5v</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/runs/zdfxdg5v' target=\"_blank\">leafy-sweep-10</a></strong> to <a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/sweeps/y5dau394' target=\"_blank\">https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/sweeps/y5dau394</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep' target=\"_blank\">https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/sweeps/y5dau394' target=\"_blank\">https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/sweeps/y5dau394</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/runs/zdfxdg5v' target=\"_blank\">https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/runs/zdfxdg5v</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Train Loss: 109.5848, Val RMSE: 1.1160, R²: -0.4533\n",
            "Epoch: 2, Train Loss: 12.1983, Val RMSE: 0.8232, R²: 0.2092\n",
            "Epoch: 3, Train Loss: 3.8030, Val RMSE: 0.7294, R²: 0.3791\n",
            "Epoch: 4, Train Loss: 1.6322, Val RMSE: 0.7002, R²: 0.4279\n",
            "Epoch: 5, Train Loss: 1.0300, Val RMSE: 0.6996, R²: 0.4288\n",
            "Epoch: 6, Train Loss: 0.8698, Val RMSE: 0.7303, R²: 0.3777\n",
            "Epoch: 7, Train Loss: 0.7946, Val RMSE: 0.6731, R²: 0.4712\n",
            "Epoch: 8, Train Loss: 0.7541, Val RMSE: 0.6934, R²: 0.4390\n",
            "Epoch: 9, Train Loss: 0.8094, Val RMSE: 0.6802, R²: 0.4601\n",
            "Epoch: 10, Train Loss: 0.6576, Val RMSE: 0.6406, R²: 0.5211\n",
            "Epoch: 11, Train Loss: 0.6221, Val RMSE: 0.6317, R²: 0.5343\n",
            "Epoch: 12, Train Loss: 0.5980, Val RMSE: 0.6204, R²: 0.5508\n",
            "Epoch: 13, Train Loss: 0.6017, Val RMSE: 0.6316, R²: 0.5344\n",
            "Epoch: 14, Train Loss: 0.5983, Val RMSE: 0.6021, R²: 0.5770\n",
            "Epoch: 15, Train Loss: 0.5811, Val RMSE: 0.5698, R²: 0.6211\n",
            "Epoch: 16, Train Loss: 0.5922, Val RMSE: 0.5693, R²: 0.6218\n",
            "Epoch: 17, Train Loss: 0.5977, Val RMSE: 0.6126, R²: 0.5621\n",
            "Epoch: 18, Train Loss: 0.5669, Val RMSE: 0.6506, R²: 0.5060\n",
            "Epoch: 19, Train Loss: 0.5634, Val RMSE: 0.6126, R²: 0.5621\n",
            "Epoch: 20, Train Loss: 0.5308, Val RMSE: 0.6062, R²: 0.5711\n",
            "Epoch: 21, Train Loss: 0.5465, Val RMSE: 0.6127, R²: 0.5619\n",
            "Epoch: 22, Train Loss: 0.5259, Val RMSE: 0.5587, R²: 0.6357\n",
            "Epoch: 23, Train Loss: 0.5337, Val RMSE: 0.5310, R²: 0.6709\n",
            "Epoch: 24, Train Loss: 0.5193, Val RMSE: 0.5807, R²: 0.6065\n",
            "Epoch: 25, Train Loss: 0.5046, Val RMSE: 0.5457, R²: 0.6525\n",
            "Epoch: 26, Train Loss: 0.5036, Val RMSE: 0.5637, R²: 0.6291\n",
            "Epoch: 27, Train Loss: 0.4920, Val RMSE: 0.5304, R²: 0.6717\n",
            "Epoch: 28, Train Loss: 0.4986, Val RMSE: 0.5315, R²: 0.6703\n",
            "Epoch: 29, Train Loss: 0.4826, Val RMSE: 0.5408, R²: 0.6587\n",
            "Epoch: 30, Train Loss: 0.4904, Val RMSE: 0.5401, R²: 0.6596\n",
            "Epoch: 31, Train Loss: 0.4581, Val RMSE: 0.5360, R²: 0.6647\n",
            "Epoch: 32, Train Loss: 0.4783, Val RMSE: 0.5345, R²: 0.6666\n",
            "Epoch: 33, Train Loss: 0.4590, Val RMSE: 0.5396, R²: 0.6602\n",
            "Epoch: 34, Train Loss: 0.4642, Val RMSE: 0.5370, R²: 0.6634\n",
            "Epoch: 35, Train Loss: 0.4474, Val RMSE: 0.5062, R²: 0.7009\n",
            "Epoch: 36, Train Loss: 0.4448, Val RMSE: 0.5201, R²: 0.6843\n",
            "Epoch: 37, Train Loss: 0.4447, Val RMSE: 0.5014, R²: 0.7066\n",
            "Epoch: 38, Train Loss: 0.4363, Val RMSE: 0.5036, R²: 0.7040\n",
            "Epoch: 39, Train Loss: 0.4382, Val RMSE: 0.5090, R²: 0.6976\n",
            "Epoch: 40, Train Loss: 0.4351, Val RMSE: 0.4978, R²: 0.7108\n",
            "Epoch: 41, Train Loss: 0.4257, Val RMSE: 0.5126, R²: 0.6933\n",
            "Epoch: 42, Train Loss: 0.4384, Val RMSE: 0.5112, R²: 0.6950\n",
            "Epoch: 43, Train Loss: 0.4238, Val RMSE: 0.5103, R²: 0.6961\n",
            "Epoch: 44, Train Loss: 0.4291, Val RMSE: 0.5081, R²: 0.6987\n",
            "Epoch: 45, Train Loss: 0.4413, Val RMSE: 0.5092, R²: 0.6974\n",
            "Epoch: 46, Train Loss: 0.4268, Val RMSE: 0.5057, R²: 0.7016\n",
            "Epoch: 47, Train Loss: 0.4271, Val RMSE: 0.5069, R²: 0.7001\n",
            "Epoch: 48, Train Loss: 0.4241, Val RMSE: 0.5075, R²: 0.6994\n",
            "Epoch: 49, Train Loss: 0.4162, Val RMSE: 0.5083, R²: 0.6985\n",
            "Epoch: 50, Train Loss: 0.4259, Val RMSE: 0.5065, R²: 0.7006\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>r2</td><td>▁▅▆▆▆▇▆▆▇▇▇▇▇▇▇▇▇██▇████████████████████</td></tr><tr><td>train_loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_rmse</td><td>█▅▄▃▃▃▃▃▃▃▃▂▂▂▃▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>50</td></tr><tr><td>r2</td><td>0.70058</td></tr><tr><td>train_loss</td><td>0.42586</td></tr><tr><td>val_rmse</td><td>0.50653</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">leafy-sweep-10</strong> at: <a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/runs/zdfxdg5v' target=\"_blank\">https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep/runs/zdfxdg5v</a><br> View project at: <a href='https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep' target=\"_blank\">https://wandb.ai/raiyann-j-university-of-toronto/GCN_Sweep</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250327_210623-zdfxdg5v/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}